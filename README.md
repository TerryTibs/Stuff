***IT NEEDS FINETUNING TO BE PERFECT***

**SACRSN BEFORE FINE TUNING**

To scale it up it will need Holographic Associative Memory

Needed to make it work better.

Explicit graph learning objective

Symbol entropy regularization

Introspection / diagnostics

Training phase schedule

Rule-level abstraction (future)

SACRSN-SAVE-ROUND1.py

Ep 0000 | Loss: 4.2455 | Avg Thinking Steps: 7.81
Ep 0010 | Loss: 3.3380 | Avg Thinking Steps: 5.39
Ep 0020 | Loss: 3.2582 | Avg Thinking Steps: 5.72
Ep 0030 | Loss: 3.2140 | Avg Thinking Steps: 5.06
Ep 0040 | Loss: 3.1292 | Avg Thinking Steps: 4.51
Ep 0050 | Loss: 3.0924 | Avg Thinking Steps: 4.27
Ep 0060 | Loss: 3.1051 | Avg Thinking Steps: 4.26
Ep 0070 | Loss: 3.1919 | Avg Thinking Steps: 4.03
Ep 0080 | Loss: 3.0788 | Avg Thinking Steps: 3.76
Ep 0090 | Loss: 3.1452 | Avg Thinking Steps: 4.59
Ep 0100 | Loss: 3.2858 | Avg Thinking Steps: 3.98
Ep 0110 | Loss: 3.3216 | Avg Thinking Steps: 4.20
Ep 0120 | Loss: 3.3262 | Avg Thinking Steps: 4.46
Ep 0130 | Loss: 3.2812 | Avg Thinking Steps: 4.44
Ep 0140 | Loss: 3.2720 | Avg Thinking Steps: 4.70
Ep 0150 | Loss: 3.2830 | Avg Thinking Steps: 4.32
Ep 0160 | Loss: 3.2146 | Avg Thinking Steps: 4.08
Ep 0170 | Loss: 3.1012 | Avg Thinking Steps: 3.61
Ep 0180 | Loss: 3.2110 | Avg Thinking Steps: 3.46
Ep 0190 | Loss: 3.3301 | Avg Thinking Steps: 3.69
Ep 0200 | Loss: 3.2724 | Avg Thinking Steps: 4.32
Ep 0210 | Loss: 3.3291 | Avg Thinking Steps: 3.93
Ep 0220 | Loss: 3.2821 | Avg Thinking Steps: 4.12
Ep 0230 | Loss: 3.2794 | Avg Thinking Steps: 4.18
Ep 0240 | Loss: 3.3225 | Avg Thinking Steps: 3.64
Ep 0250 | Loss: 3.3674 | Avg Thinking Steps: 3.64
Ep 0260 | Loss: 3.4597 | Avg Thinking Steps: 4.00
Ep 0270 | Loss: 3.1846 | Avg Thinking Steps: 3.95
Ep 0280 | Loss: 3.0486 | Avg Thinking Steps: 3.99
Ep 0290 | Loss: 3.4233 | Avg Thinking Steps: 4.30
Ep 0300 | Loss: 3.2570 | Avg Thinking Steps: 5.45
Ep 0310 | Loss: 3.1047 | Avg Thinking Steps: 5.79
Ep 0320 | Loss: 3.1103 | Avg Thinking Steps: 5.56
Ep 0330 | Loss: 3.0202 | Avg Thinking Steps: 4.80
Ep 0340 | Loss: 3.0734 | Avg Thinking Steps: 4.51
Ep 0350 | Loss: 3.0103 | Avg Thinking Steps: 4.96
Ep 0360 | Loss: 3.0937 | Avg Thinking Steps: 5.35
Ep 0370 | Loss: 2.9435 | Avg Thinking Steps: 5.42
Ep 0380 | Loss: 2.8566 | Avg Thinking Steps: 5.12
Ep 0390 | Loss: 2.8813 | Avg Thinking Steps: 4.59
Ep 0400 | Loss: 2.8291 | Avg Thinking Steps: 4.57
Ep 0410 | Loss: 2.2584 | Avg Thinking Steps: 4.10
Ep 0420 | Loss: 2.0204 | Avg Thinking Steps: 3.03
Ep 0430 | Loss: 1.6866 | Avg Thinking Steps: 2.76
Ep 0440 | Loss: 1.4404 | Avg Thinking Steps: 2.49
Ep 0450 | Loss: 1.4485 | Avg Thinking Steps: 2.35
Ep 0460 | Loss: 1.1686 | Avg Thinking Steps: 2.17
Ep 0470 | Loss: 1.1113 | Avg Thinking Steps: 2.20
Ep 0480 | Loss: 1.0931 | Avg Thinking Steps: 1.99
Ep 0490 | Loss: 0.8789 | Avg Thinking Steps: 1.84
Ep 0500 | Loss: 0.8568 | Avg Thinking Steps: 1.81
Ep 0510 | Loss: 0.8673 | Avg Thinking Steps: 1.74
Ep 0520 | Loss: 0.6889 | Avg Thinking Steps: 1.66
Ep 0530 | Loss: 0.8927 | Avg Thinking Steps: 1.75
Ep 0540 | Loss: 0.8648 | Avg Thinking Steps: 1.68
Ep 0550 | Loss: 0.5942 | Avg Thinking Steps: 1.62
Ep 0560 | Loss: 0.6803 | Avg Thinking Steps: 1.61
Ep 0570 | Loss: 0.6008 | Avg Thinking Steps: 1.61
Ep 0580 | Loss: 0.6761 | Avg Thinking Steps: 1.55
Ep 0590 | Loss: 0.5408 | Avg Thinking Steps: 1.50
Ep 0600 | Loss: 0.5790 | Avg Thinking Steps: 1.56
Ep 0610 | Loss: 0.6358 | Avg Thinking Steps: 1.54
Ep 0620 | Loss: 0.5107 | Avg Thinking Steps: 1.47
Ep 0630 | Loss: 0.4852 | Avg Thinking Steps: 1.47
Ep 0640 | Loss: 0.5705 | Avg Thinking Steps: 1.47
Ep 0650 | Loss: 0.5733 | Avg Thinking Steps: 1.46
Ep 0660 | Loss: 0.4587 | Avg Thinking Steps: 1.43
Ep 0670 | Loss: 0.4779 | Avg Thinking Steps: 1.40
Ep 0680 | Loss: 0.3337 | Avg Thinking Steps: 1.40
Ep 0690 | Loss: 0.5114 | Avg Thinking Steps: 1.43
Ep 0700 | Loss: 0.3907 | Avg Thinking Steps: 1.47
Ep 0710 | Loss: 0.4800 | Avg Thinking Steps: 1.47
Ep 0720 | Loss: 0.4716 | Avg Thinking Steps: 1.45
Ep 0730 | Loss: 0.3548 | Avg Thinking Steps: 1.45
Ep 0740 | Loss: 0.3599 | Avg Thinking Steps: 1.43
Ep 0750 | Loss: 0.3600 | Avg Thinking Steps: 1.45
Ep 0760 | Loss: 0.3338 | Avg Thinking Steps: 1.45
Ep 0770 | Loss: 0.4213 | Avg Thinking Steps: 1.50
Ep 0780 | Loss: 0.4751 | Avg Thinking Steps: 1.46
Ep 0790 | Loss: 0.3957 | Avg Thinking Steps: 1.43
Ep 0800 | Loss: 0.3751 | Avg Thinking Steps: 1.42
Ep 0810 | Loss: 0.2841 | Avg Thinking Steps: 1.40
Ep 0820 | Loss: 0.4040 | Avg Thinking Steps: 1.39
Ep 0830 | Loss: 0.3935 | Avg Thinking Steps: 1.39
Ep 0840 | Loss: 0.2595 | Avg Thinking Steps: 1.40
Ep 0850 | Loss: 0.2308 | Avg Thinking Steps: 1.40
Ep 0860 | Loss: 0.2622 | Avg Thinking Steps: 1.43
Ep 0870 | Loss: 0.3580 | Avg Thinking Steps: 1.42
Ep 0880 | Loss: 0.2711 | Avg Thinking Steps: 1.34
Ep 0890 | Loss: 0.3259 | Avg Thinking Steps: 1.35
Ep 0900 | Loss: 0.2719 | Avg Thinking Steps: 1.37
Ep 0910 | Loss: 0.2672 | Avg Thinking Steps: 1.38
Ep 0920 | Loss: 0.2466 | Avg Thinking Steps: 1.36
Ep 0930 | Loss: 0.2648 | Avg Thinking Steps: 1.32
Ep 0940 | Loss: 0.3150 | Avg Thinking Steps: 1.38
Ep 0950 | Loss: 0.2705 | Avg Thinking Steps: 1.38
Ep 0960 | Loss: 0.3303 | Avg Thinking Steps: 1.39
Ep 0970 | Loss: 0.2342 | Avg Thinking Steps: 1.37
Ep 0980 | Loss: 0.2579 | Avg Thinking Steps: 1.37
Ep 0990 | Loss: 0.2242 | Avg Thinking Steps: 1.36

--- Saving Model to crsn_complex_model.pth ---
Saved.
Model saved locally to /home/dan/Desktop/ProjectX/crsn_complex_model.pth

--- Visualizing Learned Symbolic Graph ---
Graph sparse.

Generated: That which is ention into earth.i
That which is below is below is above.
Its force ole  that which is

**FORCED GRAPH CREATION**
<img width="690" height="803" alt="Figure_1" src="https://github.com/user-attachments/assets/71cfa4ae-5127-4be7-95cc-8c22cea8dcf2" />

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

**SACRSN AFTER FINE TUNING**

***SACRSN-FINETUNE-ROUND2.py***

Ep 1000 | Loss: 0.1332 | Avg Steps: 1.35
Ep 1010 | Loss: 0.0985 | Avg Steps: 1.36
Ep 1020 | Loss: 0.1015 | Avg Steps: 1.37
Ep 1030 | Loss: 0.0981 | Avg Steps: 1.38
Ep 1040 | Loss: 0.1061 | Avg Steps: 1.37
Ep 1050 | Loss: 0.1415 | Avg Steps: 1.38
Ep 1060 | Loss: 0.1592 | Avg Steps: 1.39
Ep 1070 | Loss: 0.0972 | Avg Steps: 1.38
Ep 1080 | Loss: 0.1892 | Avg Steps: 1.41
Ep 1090 | Loss: 0.1110 | Avg Steps: 1.40
Ep 1100 | Loss: 0.1032 | Avg Steps: 1.40
Ep 1110 | Loss: 0.1272 | Avg Steps: 1.39
Ep 1120 | Loss: 0.1054 | Avg Steps: 1.39
Ep 1130 | Loss: 0.1136 | Avg Steps: 1.40
Ep 1140 | Loss: 0.1050 | Avg Steps: 1.41
Ep 1150 | Loss: 0.1046 | Avg Steps: 1.42
Ep 1160 | Loss: 0.0966 | Avg Steps: 1.42
Ep 1170 | Loss: 0.0953 | Avg Steps: 1.45
Ep 1180 | Loss: 0.0930 | Avg Steps: 1.44
Ep 1190 | Loss: 0.1037 | Avg Steps: 1.46
Ep 1200 | Loss: 0.1776 | Avg Steps: 1.46
Ep 1210 | Loss: 0.1115 | Avg Steps: 1.45
Ep 1220 | Loss: 0.1016 | Avg Steps: 1.42
Ep 1230 | Loss: 0.0970 | Avg Steps: 1.42
Ep 1240 | Loss: 0.1022 | Avg Steps: 1.42
Ep 1250 | Loss: 0.0958 | Avg Steps: 1.45
Ep 1260 | Loss: 0.1004 | Avg Steps: 1.46
Ep 1270 | Loss: 0.0967 | Avg Steps: 1.43
Ep 1280 | Loss: 0.0892 | Avg Steps: 1.43
Ep 1290 | Loss: 0.0893 | Avg Steps: 1.44
Ep 1300 | Loss: 0.0888 | Avg Steps: 1.44
Ep 1310 | Loss: 0.0983 | Avg Steps: 1.43
Ep 1320 | Loss: 0.0933 | Avg Steps: 1.46
Ep 1330 | Loss: 0.0944 | Avg Steps: 1.46
Ep 1340 | Loss: 0.1108 | Avg Steps: 1.45
Ep 1350 | Loss: 0.1015 | Avg Steps: 1.45
Ep 1360 | Loss: 0.0984 | Avg Steps: 1.43
Ep 1370 | Loss: 0.0991 | Avg Steps: 1.44
Ep 1380 | Loss: 0.0976 | Avg Steps: 1.44
Ep 1390 | Loss: 0.0957 | Avg Steps: 1.42
Ep 1400 | Loss: 0.1047 | Avg Steps: 1.41
Ep 1410 | Loss: 0.1068 | Avg Steps: 1.42
Ep 1420 | Loss: 0.1094 | Avg Steps: 1.42
Ep 1430 | Loss: 0.0972 | Avg Steps: 1.43
Ep 1440 | Loss: 0.1069 | Avg Steps: 1.43
Ep 1450 | Loss: 0.0970 | Avg Steps: 1.42
Ep 1460 | Loss: 0.0973 | Avg Steps: 1.43
Ep 1470 | Loss: 0.1081 | Avg Steps: 1.41
Ep 1480 | Loss: 0.1329 | Avg Steps: 1.40
Ep 1490 | Loss: 0.0829 | Avg Steps: 1.39

--- Saving Model to crsn_complex_model.pth ---
Saved.

--- Visualizing Learned Symbolic Graph ---

Generated: That which is below is like to that which is above.
The  of it be world is like to that which is above.
The  t all pentir of all perthout fals is like

<img width="690" height="803" alt="Figure_2" src="https://github.com/user-attachments/assets/fe8382da-21c7-4d22-aa31-122b5e9f319b" />

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

WITH CONFIG 

"embedding_dim": 64

"n_symbols": 64

"act_threshold": 0.9999

Ep 0000 | Loss: 4.1463 | Avg Thinking Steps: 7.72
Ep 0010 | Loss: 3.2999 | Avg Thinking Steps: 5.89
Ep 0020 | Loss: 3.1952 | Avg Thinking Steps: 5.16
Ep 0030 | Loss: 3.2096 | Avg Thinking Steps: 4.80
Ep 0040 | Loss: 3.2536 | Avg Thinking Steps: 4.43
Ep 0050 | Loss: 3.1265 | Avg Thinking Steps: 4.14
Ep 0060 | Loss: 3.1977 | Avg Thinking Steps: 3.91
Ep 0070 | Loss: 3.2302 | Avg Thinking Steps: 3.97
Ep 0080 | Loss: 3.2078 | Avg Thinking Steps: 3.84
Ep 0090 | Loss: 3.3837 | Avg Thinking Steps: 3.49
Ep 0100 | Loss: 3.2556 | Avg Thinking Steps: 3.57
Ep 0110 | Loss: 3.3830 | Avg Thinking Steps: 3.89
Ep 0120 | Loss: 3.2321 | Avg Thinking Steps: 3.82
Ep 0130 | Loss: 3.2922 | Avg Thinking Steps: 3.89
Ep 0140 | Loss: 3.2203 | Avg Thinking Steps: 3.99
Ep 0150 | Loss: 3.3150 | Avg Thinking Steps: 3.85
Ep 0160 | Loss: 3.1785 | Avg Thinking Steps: 4.04
Ep 0170 | Loss: 3.2579 | Avg Thinking Steps: 4.10
Ep 0180 | Loss: 3.3079 | Avg Thinking Steps: 4.26
Ep 0190 | Loss: 3.2042 | Avg Thinking Steps: 4.18
Ep 0200 | Loss: 3.2819 | Avg Thinking Steps: 4.37
Ep 0210 | Loss: 3.2289 | Avg Thinking Steps: 3.69
Ep 0220 | Loss: 3.3474 | Avg Thinking Steps: 3.52
Ep 0230 | Loss: 3.3092 | Avg Thinking Steps: 3.53
Ep 0240 | Loss: 3.4275 | Avg Thinking Steps: 3.50
Ep 0250 | Loss: 3.3770 | Avg Thinking Steps: 3.49
Ep 0260 | Loss: 3.3584 | Avg Thinking Steps: 3.64
Ep 0270 | Loss: 3.3166 | Avg Thinking Steps: 3.74
Ep 0280 | Loss: 3.2917 | Avg Thinking Steps: 3.89
Ep 0290 | Loss: 3.4637 | Avg Thinking Steps: 4.12
Ep 0300 | Loss: 3.2821 | Avg Thinking Steps: 4.37
Ep 0310 | Loss: 3.2676 | Avg Thinking Steps: 4.49
Ep 0320 | Loss: 3.4065 | Avg Thinking Steps: 4.44
Ep 0330 | Loss: 3.1763 | Avg Thinking Steps: 4.88
Ep 0340 | Loss: 3.2152 | Avg Thinking Steps: 5.16
Ep 0350 | Loss: 3.2274 | Avg Thinking Steps: 4.61
Ep 0360 | Loss: 3.0223 | Avg Thinking Steps: 4.28
Ep 0370 | Loss: 3.0863 | Avg Thinking Steps: 4.53
Ep 0380 | Loss: 3.0290 | Avg Thinking Steps: 4.57
Ep 0390 | Loss: 2.8823 | Avg Thinking Steps: 4.04
Ep 0400 | Loss: 2.8484 | Avg Thinking Steps: 3.58
Ep 0410 | Loss: 2.8141 | Avg Thinking Steps: 3.37
Ep 0420 | Loss: 2.5425 | Avg Thinking Steps: 3.78
Ep 0430 | Loss: 2.4119 | Avg Thinking Steps: 3.46
Ep 0440 | Loss: 2.2194 | Avg Thinking Steps: 3.13
Ep 0450 | Loss: 2.1420 | Avg Thinking Steps: 2.99
Ep 0460 | Loss: 1.8607 | Avg Thinking Steps: 2.61
Ep 0470 | Loss: 1.7362 | Avg Thinking Steps: 2.40
Ep 0480 | Loss: 1.5393 | Avg Thinking Steps: 2.09
Ep 0490 | Loss: 1.5452 | Avg Thinking Steps: 1.99
Ep 0500 | Loss: 1.4882 | Avg Thinking Steps: 1.80
Ep 0510 | Loss: 1.3191 | Avg Thinking Steps: 1.87
Ep 0520 | Loss: 1.3726 | Avg Thinking Steps: 1.86
Ep 0530 | Loss: 1.2178 | Avg Thinking Steps: 1.80
Ep 0540 | Loss: 1.2705 | Avg Thinking Steps: 1.79
Ep 0550 | Loss: 1.1117 | Avg Thinking Steps: 1.69
Ep 0560 | Loss: 1.1896 | Avg Thinking Steps: 1.68
Ep 0570 | Loss: 1.0855 | Avg Thinking Steps: 1.66
Ep 0580 | Loss: 0.9837 | Avg Thinking Steps: 1.57
Ep 0590 | Loss: 0.9894 | Avg Thinking Steps: 1.61
Ep 0600 | Loss: 0.9142 | Avg Thinking Steps: 1.49
Ep 0610 | Loss: 0.8895 | Avg Thinking Steps: 1.51
Ep 0620 | Loss: 0.7123 | Avg Thinking Steps: 1.53
Ep 0630 | Loss: 0.7905 | Avg Thinking Steps: 1.50
Ep 0640 | Loss: 0.6758 | Avg Thinking Steps: 1.47
Ep 0650 | Loss: 0.8428 | Avg Thinking Steps: 1.47
Ep 0660 | Loss: 0.8095 | Avg Thinking Steps: 1.44
Ep 0670 | Loss: 0.8879 | Avg Thinking Steps: 1.39
Ep 0680 | Loss: 0.5772 | Avg Thinking Steps: 1.36
Ep 0690 | Loss: 0.6151 | Avg Thinking Steps: 1.38
Ep 0700 | Loss: 0.5386 | Avg Thinking Steps: 1.36
Ep 0710 | Loss: 0.7221 | Avg Thinking Steps: 1.40
Ep 0720 | Loss: 0.6177 | Avg Thinking Steps: 1.37
Ep 0730 | Loss: 0.3857 | Avg Thinking Steps: 1.34
Ep 0740 | Loss: 0.3522 | Avg Thinking Steps: 1.35
Ep 0750 | Loss: 0.6779 | Avg Thinking Steps: 1.34
Ep 0760 | Loss: 0.5240 | Avg Thinking Steps: 1.36
Ep 0770 | Loss: 0.4370 | Avg Thinking Steps: 1.33
Ep 0780 | Loss: 0.4510 | Avg Thinking Steps: 1.31
Ep 0790 | Loss: 0.4578 | Avg Thinking Steps: 1.29
Ep 0800 | Loss: 0.5209 | Avg Thinking Steps: 1.25
Ep 0810 | Loss: 0.4249 | Avg Thinking Steps: 1.30
Ep 0820 | Loss: 0.5609 | Avg Thinking Steps: 1.34
Ep 0830 | Loss: 0.5570 | Avg Thinking Steps: 1.27
Ep 0840 | Loss: 0.6495 | Avg Thinking Steps: 1.27
Ep 0850 | Loss: 0.4904 | Avg Thinking Steps: 1.24
Ep 0860 | Loss: 0.3993 | Avg Thinking Steps: 1.28
Ep 0870 | Loss: 0.4296 | Avg Thinking Steps: 1.24
Ep 0880 | Loss: 0.3398 | Avg Thinking Steps: 1.19
Ep 0890 | Loss: 0.3763 | Avg Thinking Steps: 1.17
Ep 0900 | Loss: 0.3979 | Avg Thinking Steps: 1.19
Ep 0910 | Loss: 0.3172 | Avg Thinking Steps: 1.14
Ep 0920 | Loss: 0.3603 | Avg Thinking Steps: 1.19
Ep 0930 | Loss: 0.3342 | Avg Thinking Steps: 1.18
Ep 0940 | Loss: 0.3476 | Avg Thinking Steps: 1.15
Ep 0950 | Loss: 0.3108 | Avg Thinking Steps: 1.18
Ep 0960 | Loss: 0.3129 | Avg Thinking Steps: 1.16
Ep 0970 | Loss: 0.2867 | Avg Thinking Steps: 1.17
Ep 0980 | Loss: 0.2550 | Avg Thinking Steps: 1.18
Ep 0990 | Loss: 0.2854 | Avg Thinking Steps: 1.16
Ep 1000 | Loss: 0.2378 | Avg Thinking Steps: 1.15
Ep 1010 | Loss: 0.4887 | Avg Thinking Steps: 1.15
Ep 1020 | Loss: 0.3846 | Avg Thinking Steps: 1.16
Ep 1030 | Loss: 0.2437 | Avg Thinking Steps: 1.15
Ep 1040 | Loss: 0.4449 | Avg Thinking Steps: 1.15
Ep 1050 | Loss: 0.2638 | Avg Thinking Steps: 1.16
Ep 1060 | Loss: 0.2433 | Avg Thinking Steps: 1.18
Ep 1070 | Loss: 0.2647 | Avg Thinking Steps: 1.19
Ep 1080 | Loss: 0.3438 | Avg Thinking Steps: 1.16
Ep 1090 | Loss: 0.2386 | Avg Thinking Steps: 1.17
Ep 1100 | Loss: 0.2360 | Avg Thinking Steps: 1.14
Ep 1110 | Loss: 0.3319 | Avg Thinking Steps: 1.15
Ep 1120 | Loss: 0.3374 | Avg Thinking Steps: 1.18
Ep 1130 | Loss: 0.2241 | Avg Thinking Steps: 1.12
Ep 1140 | Loss: 0.2130 | Avg Thinking Steps: 1.13
Ep 1150 | Loss: 0.2242 | Avg Thinking Steps: 1.15
Ep 1160 | Loss: 0.3069 | Avg Thinking Steps: 1.14
Ep 1170 | Loss: 0.2664 | Avg Thinking Steps: 1.12
Ep 1180 | Loss: 0.2081 | Avg Thinking Steps: 1.16
Ep 1190 | Loss: 0.2116 | Avg Thinking Steps: 1.16
Ep 1200 | Loss: 0.1743 | Avg Thinking Steps: 1.15
Ep 1210 | Loss: 0.3965 | Avg Thinking Steps: 1.18
Ep 1220 | Loss: 0.2694 | Avg Thinking Steps: 1.18
Ep 1230 | Loss: 0.2684 | Avg Thinking Steps: 1.17
Ep 1240 | Loss: 0.2106 | Avg Thinking Steps: 1.13
Ep 1250 | Loss: 0.2049 | Avg Thinking Steps: 1.14
Ep 1260 | Loss: 0.1845 | Avg Thinking Steps: 1.12
Ep 1270 | Loss: 0.1907 | Avg Thinking Steps: 1.12
Ep 1280 | Loss: 0.1977 | Avg Thinking Steps: 1.12
Ep 1290 | Loss: 0.2752 | Avg Thinking Steps: 1.12
Ep 1300 | Loss: 0.2416 | Avg Thinking Steps: 1.13
Ep 1310 | Loss: 0.1528 | Avg Thinking Steps: 1.12
Ep 1320 | Loss: 0.1764 | Avg Thinking Steps: 1.13
Ep 1330 | Loss: 0.2057 | Avg Thinking Steps: 1.16
Ep 1340 | Loss: 0.1932 | Avg Thinking Steps: 1.13
Ep 1350 | Loss: 0.2026 | Avg Thinking Steps: 1.12
Ep 1360 | Loss: 0.2182 | Avg Thinking Steps: 1.13
Ep 1370 | Loss: 0.1782 | Avg Thinking Steps: 1.13
Ep 1380 | Loss: 0.1478 | Avg Thinking Steps: 1.13
Ep 1390 | Loss: 0.1615 | Avg Thinking Steps: 1.16
Ep 1400 | Loss: 0.1776 | Avg Thinking Steps: 1.16
Ep 1410 | Loss: 0.2250 | Avg Thinking Steps: 1.15
Ep 1420 | Loss: 0.2510 | Avg Thinking Steps: 1.16
Ep 1430 | Loss: 0.1701 | Avg Thinking Steps: 1.15
Ep 1440 | Loss: 0.1741 | Avg Thinking Steps: 1.14
Ep 1450 | Loss: 0.1324 | Avg Thinking Steps: 1.16
Ep 1460 | Loss: 0.2329 | Avg Thinking Steps: 1.19
Ep 1470 | Loss: 0.1498 | Avg Thinking Steps: 1.19
Ep 1480 | Loss: 0.2097 | Avg Thinking Steps: 1.13
Ep 1490 | Loss: 0.1910 | Avg Thinking Steps: 1.14
Ep 1500 | Loss: 0.2217 | Avg Thinking Steps: 1.17
Ep 1510 | Loss: 0.1543 | Avg Thinking Steps: 1.18
Ep 1520 | Loss: 0.2496 | Avg Thinking Steps: 1.19
Ep 1530 | Loss: 0.1385 | Avg Thinking Steps: 1.16
Ep 1540 | Loss: 0.3616 | Avg Thinking Steps: 1.16
Ep 1550 | Loss: 0.2149 | Avg Thinking Steps: 1.19
Ep 1560 | Loss: 0.1697 | Avg Thinking Steps: 1.18
Ep 1570 | Loss: 0.1857 | Avg Thinking Steps: 1.17
Ep 1580 | Loss: 0.2507 | Avg Thinking Steps: 1.14
Ep 1590 | Loss: 0.1713 | Avg Thinking Steps: 1.16
Ep 1600 | Loss: 0.1334 | Avg Thinking Steps: 1.16
Ep 1610 | Loss: 0.2253 | Avg Thinking Steps: 1.14
Ep 1620 | Loss: 0.2330 | Avg Thinking Steps: 1.13
Ep 1630 | Loss: 0.2012 | Avg Thinking Steps: 1.16
Ep 1640 | Loss: 0.2925 | Avg Thinking Steps: 1.17
Ep 1650 | Loss: 0.2042 | Avg Thinking Steps: 1.13
Ep 1660 | Loss: 0.1959 | Avg Thinking Steps: 1.14
Ep 1670 | Loss: 0.1577 | Avg Thinking Steps: 1.14
Ep 1680 | Loss: 0.1777 | Avg Thinking Steps: 1.15
Ep 1690 | Loss: 0.2823 | Avg Thinking Steps: 1.15
Ep 1700 | Loss: 0.1729 | Avg Thinking Steps: 1.14
Ep 1710 | Loss: 0.2550 | Avg Thinking Steps: 1.13
Ep 1720 | Loss: 0.1162 | Avg Thinking Steps: 1.16
Ep 1730 | Loss: 0.1342 | Avg Thinking Steps: 1.18
Ep 1740 | Loss: 0.1492 | Avg Thinking Steps: 1.17
Ep 1750 | Loss: 0.2259 | Avg Thinking Steps: 1.15
Ep 1760 | Loss: 0.2343 | Avg Thinking Steps: 1.18
Ep 1770 | Loss: 0.1889 | Avg Thinking Steps: 1.16
Ep 1780 | Loss: 0.1986 | Avg Thinking Steps: 1.14
Ep 1790 | Loss: 0.1844 | Avg Thinking Steps: 1.12
Ep 1800 | Loss: 0.3049 | Avg Thinking Steps: 1.17
Ep 1810 | Loss: 0.1718 | Avg Thinking Steps: 1.16
Ep 1820 | Loss: 0.1268 | Avg Thinking Steps: 1.16
Ep 1830 | Loss: 0.2173 | Avg Thinking Steps: 1.17
Ep 1840 | Loss: 0.1749 | Avg Thinking Steps: 1.16
Ep 1850 | Loss: 0.2013 | Avg Thinking Steps: 1.16
Ep 1860 | Loss: 0.2058 | Avg Thinking Steps: 1.20
Ep 1870 | Loss: 0.2986 | Avg Thinking Steps: 1.18
Ep 1880 | Loss: 0.1773 | Avg Thinking Steps: 1.15
Ep 1890 | Loss: 0.1582 | Avg Thinking Steps: 1.18
Ep 1900 | Loss: 0.1876 | Avg Thinking Steps: 1.18
Ep 1910 | Loss: 0.2478 | Avg Thinking Steps: 1.18
Ep 1920 | Loss: 0.1592 | Avg Thinking Steps: 1.15
Ep 1930 | Loss: 0.3353 | Avg Thinking Steps: 1.16
Ep 1940 | Loss: 0.1335 | Avg Thinking Steps: 1.16
Ep 1950 | Loss: 0.1265 | Avg Thinking Steps: 1.15
Ep 1960 | Loss: 0.1640 | Avg Thinking Steps: 1.16
Ep 1970 | Loss: 0.1897 | Avg Thinking Steps: 1.15
Ep 1980 | Loss: 0.2445 | Avg Thinking Steps: 1.14
Ep 1990 | Loss: 0.1571 | Avg Thinking Steps: 1.18
Ep 2000 | Loss: 0.1958 | Avg Thinking Steps: 1.17
Ep 2010 | Loss: 0.1716 | Avg Thinking Steps: 1.15
Ep 2020 | Loss: 0.2643 | Avg Thinking Steps: 1.16
Ep 2030 | Loss: 0.2856 | Avg Thinking Steps: 1.16
Ep 2040 | Loss: 0.2441 | Avg Thinking Steps: 1.16
Ep 2050 | Loss: 0.1810 | Avg Thinking Steps: 1.16
Ep 2060 | Loss: 0.2133 | Avg Thinking Steps: 1.16
Ep 2070 | Loss: 0.2883 | Avg Thinking Steps: 1.16
Ep 2080 | Loss: 0.2229 | Avg Thinking Steps: 1.13
Ep 2090 | Loss: 0.1826 | Avg Thinking Steps: 1.14
Ep 2100 | Loss: 0.1400 | Avg Thinking Steps: 1.16
Ep 2110 | Loss: 0.1458 | Avg Thinking Steps: 1.14
Ep 2120 | Loss: 0.1946 | Avg Thinking Steps: 1.14
Ep 2130 | Loss: 0.1935 | Avg Thinking Steps: 1.13
Ep 2140 | Loss: 0.2591 | Avg Thinking Steps: 1.17
Ep 2150 | Loss: 0.1648 | Avg Thinking Steps: 1.14
Ep 2160 | Loss: 0.1599 | Avg Thinking Steps: 1.14
Ep 2170 | Loss: 0.2571 | Avg Thinking Steps: 1.14
Ep 2180 | Loss: 0.1660 | Avg Thinking Steps: 1.15
Ep 2190 | Loss: 0.2333 | Avg Thinking Steps: 1.12
Ep 2200 | Loss: 0.2301 | Avg Thinking Steps: 1.16
Ep 2210 | Loss: 0.1918 | Avg Thinking Steps: 1.13
Ep 2220 | Loss: 0.2368 | Avg Thinking Steps: 1.14
Ep 2230 | Loss: 0.3319 | Avg Thinking Steps: 1.16
Ep 2240 | Loss: 0.2509 | Avg Thinking Steps: 1.15
Ep 2250 | Loss: 0.1536 | Avg Thinking Steps: 1.13
Ep 2260 | Loss: 0.2584 | Avg Thinking Steps: 1.12
Ep 2270 | Loss: 0.2086 | Avg Thinking Steps: 1.15
Ep 2280 | Loss: 0.2886 | Avg Thinking Steps: 1.15
Ep 2290 | Loss: 0.1643 | Avg Thinking Steps: 1.14
Ep 2300 | Loss: 0.2972 | Avg Thinking Steps: 1.18
Ep 2310 | Loss: 0.2425 | Avg Thinking Steps: 1.18
Ep 2320 | Loss: 0.1373 | Avg Thinking Steps: 1.14
Ep 2330 | Loss: 0.2105 | Avg Thinking Steps: 1.12
Ep 2340 | Loss: 0.1733 | Avg Thinking Steps: 1.12
Ep 2350 | Loss: 0.2441 | Avg Thinking Steps: 1.14
Ep 2360 | Loss: 0.1575 | Avg Thinking Steps: 1.16
Ep 2370 | Loss: 0.1109 | Avg Thinking Steps: 1.12
Ep 2380 | Loss: 0.2289 | Avg Thinking Steps: 1.12
Ep 2390 | Loss: 0.1766 | Avg Thinking Steps: 1.15
Ep 2400 | Loss: 0.1207 | Avg Thinking Steps: 1.14
Ep 2410 | Loss: 0.1487 | Avg Thinking Steps: 1.13
Ep 2420 | Loss: 0.2315 | Avg Thinking Steps: 1.15
Ep 2430 | Loss: 0.1939 | Avg Thinking Steps: 1.12
Ep 2440 | Loss: 0.3793 | Avg Thinking Steps: 1.13
Ep 2450 | Loss: 0.1695 | Avg Thinking Steps: 1.14
Ep 2460 | Loss: 0.2461 | Avg Thinking Steps: 1.15
Ep 2470 | Loss: 0.1785 | Avg Thinking Steps: 1.16
Ep 2480 | Loss: 0.3012 | Avg Thinking Steps: 1.15
Ep 2490 | Loss: 0.1884 | Avg Thinking Steps: 1.16
Ep 2500 | Loss: 0.1743 | Avg Thinking Steps: 1.16
Ep 2510 | Loss: 0.1435 | Avg Thinking Steps: 1.14
Ep 2520 | Loss: 0.1730 | Avg Thinking Steps: 1.15
Ep 2530 | Loss: 0.1073 | Avg Thinking Steps: 1.11
Ep 2540 | Loss: 0.1503 | Avg Thinking Steps: 1.14
Ep 2550 | Loss: 0.2048 | Avg Thinking Steps: 1.13
Ep 2560 | Loss: 0.4428 | Avg Thinking Steps: 1.12
Ep 2570 | Loss: 0.1777 | Avg Thinking Steps: 1.12
Ep 2580 | Loss: 0.2939 | Avg Thinking Steps: 1.15
Ep 2590 | Loss: 0.2735 | Avg Thinking Steps: 1.16
Ep 2600 | Loss: 0.1891 | Avg Thinking Steps: 1.15
Ep 2610 | Loss: 0.2162 | Avg Thinking Steps: 1.11
Ep 2620 | Loss: 0.2425 | Avg Thinking Steps: 1.14
Ep 2630 | Loss: 0.2042 | Avg Thinking Steps: 1.13
Ep 2640 | Loss: 0.2316 | Avg Thinking Steps: 1.12
Ep 2650 | Loss: 0.2120 | Avg Thinking Steps: 1.13
Ep 2660 | Loss: 0.2342 | Avg Thinking Steps: 1.13
Ep 2670 | Loss: 0.1446 | Avg Thinking Steps: 1.12
Ep 2680 | Loss: 0.1432 | Avg Thinking Steps: 1.13
Ep 2690 | Loss: 0.1535 | Avg Thinking Steps: 1.12
Ep 2700 | Loss: 0.2174 | Avg Thinking Steps: 1.18
Ep 2710 | Loss: 0.2573 | Avg Thinking Steps: 1.12
Ep 2720 | Loss: 0.1409 | Avg Thinking Steps: 1.11
Ep 2730 | Loss: 0.1872 | Avg Thinking Steps: 1.14
Ep 2740 | Loss: 0.2407 | Avg Thinking Steps: 1.13
Ep 2750 | Loss: 0.2229 | Avg Thinking Steps: 1.11
Ep 2760 | Loss: 0.1515 | Avg Thinking Steps: 1.11
Ep 2770 | Loss: 0.1928 | Avg Thinking Steps: 1.09
Ep 2780 | Loss: 0.1744 | Avg Thinking Steps: 1.13
Ep 2790 | Loss: 0.2417 | Avg Thinking Steps: 1.13
Ep 2800 | Loss: 0.2871 | Avg Thinking Steps: 1.12
Ep 2810 | Loss: 0.2519 | Avg Thinking Steps: 1.16
Ep 2820 | Loss: 0.1498 | Avg Thinking Steps: 1.15
Ep 2830 | Loss: 0.2401 | Avg Thinking Steps: 1.16
Ep 2840 | Loss: 0.1819 | Avg Thinking Steps: 1.16
Ep 2850 | Loss: 0.2822 | Avg Thinking Steps: 1.14
Ep 2860 | Loss: 0.1970 | Avg Thinking Steps: 1.18
Ep 2870 | Loss: 0.2623 | Avg Thinking Steps: 1.18
Ep 2880 | Loss: 0.4636 | Avg Thinking Steps: 1.17
Ep 2890 | Loss: 0.2438 | Avg Thinking Steps: 1.14
Ep 2900 | Loss: 0.2291 | Avg Thinking Steps: 1.16
Ep 2910 | Loss: 0.1788 | Avg Thinking Steps: 1.18
Ep 2920 | Loss: 0.3098 | Avg Thinking Steps: 1.18
Ep 2930 | Loss: 0.3122 | Avg Thinking Steps: 1.19
Ep 2940 | Loss: 0.1694 | Avg Thinking Steps: 1.14
Ep 2950 | Loss: 0.1698 | Avg Thinking Steps: 1.15
Ep 2960 | Loss: 0.3085 | Avg Thinking Steps: 1.15
Ep 2970 | Loss: 0.3880 | Avg Thinking Steps: 1.13
Ep 2980 | Loss: 0.2262 | Avg Thinking Steps: 1.14
Ep 2990 | Loss: 0.1784 | Avg Thinking Steps: 1.12

--- Saving Model to crsn_complex_model.pth ---
Saved.
Model saved locally to /home/dan/Desktop/ProjectX/crsn_complex_model.pth

--- Visualizing Learned Symbolic Graph ---
Graph sparse.

Generated: That which is below is he is hich is above.
The is like to that which is above.
The in the is like to

------------------------------------------------------------------------------------------------------------------------------------------

WITH CONFIG

    "seq_len": 32,
    "embedding_dim": 64,
    "max_recursion_depth": 8,
    "act_threshold": 0.9999,
    "ponder_penalty": 0.0001,  
    
    "n_symbols": 64,
    "n_concepts": 8,
    "commitment_cost": 0.25,
    "graph_bias_scale": 0.5, 
    
    "epochs": 3000,
    "learning_rate": 0.001,   
    "grad_clip": 0.5,         
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "eps": 1e-6


Ep 0000 | Loss: 4.2225 | Avg Thinking Steps: 7.91
Ep 0010 | Loss: 3.2207 | Avg Thinking Steps: 4.69
Ep 0020 | Loss: 3.1819 | Avg Thinking Steps: 3.93
Ep 0030 | Loss: 3.1492 | Avg Thinking Steps: 3.88
Ep 0040 | Loss: 3.1032 | Avg Thinking Steps: 3.63
Ep 0050 | Loss: 3.1253 | Avg Thinking Steps: 3.34
Ep 0060 | Loss: 3.2009 | Avg Thinking Steps: 3.41
Ep 0070 | Loss: 3.1578 | Avg Thinking Steps: 3.71
Ep 0080 | Loss: 3.2224 | Avg Thinking Steps: 3.66
Ep 0090 | Loss: 3.1205 | Avg Thinking Steps: 3.86
Ep 0100 | Loss: 3.2945 | Avg Thinking Steps: 3.51
Ep 0110 | Loss: 3.1327 | Avg Thinking Steps: 3.72
Ep 0120 | Loss: 3.1982 | Avg Thinking Steps: 3.27
Ep 0130 | Loss: 3.1521 | Avg Thinking Steps: 3.34
Ep 0140 | Loss: 3.1695 | Avg Thinking Steps: 3.86
Ep 0150 | Loss: 3.2196 | Avg Thinking Steps: 3.97
Ep 0160 | Loss: 3.1425 | Avg Thinking Steps: 3.40
Ep 0170 | Loss: 3.2593 | Avg Thinking Steps: 3.54
Ep 0180 | Loss: 3.3448 | Avg Thinking Steps: 4.34
Ep 0190 | Loss: 3.4425 | Avg Thinking Steps: 4.26
Ep 0200 | Loss: 3.4090 | Avg Thinking Steps: 4.45
Ep 0210 | Loss: 3.4563 | Avg Thinking Steps: 3.86
Ep 0220 | Loss: 3.4059 | Avg Thinking Steps: 3.78
Ep 0230 | Loss: 3.2391 | Avg Thinking Steps: 3.82
Ep 0240 | Loss: 3.0874 | Avg Thinking Steps: 3.90
Ep 0250 | Loss: 3.0555 | Avg Thinking Steps: 5.71
Ep 0260 | Loss: 3.0880 | Avg Thinking Steps: 3.98
Ep 0270 | Loss: 3.1689 | Avg Thinking Steps: 4.08
Ep 0280 | Loss: 3.2651 | Avg Thinking Steps: 3.52
Ep 0290 | Loss: 3.4609 | Avg Thinking Steps: 3.44
Ep 0300 | Loss: 3.4425 | Avg Thinking Steps: 3.22
Ep 0310 | Loss: 3.3266 | Avg Thinking Steps: 3.48
Ep 0320 | Loss: 3.3543 | Avg Thinking Steps: 4.17
Ep 0330 | Loss: 3.2233 | Avg Thinking Steps: 4.42
Ep 0340 | Loss: 3.3059 | Avg Thinking Steps: 4.73
Ep 0350 | Loss: 3.2704 | Avg Thinking Steps: 4.57
Ep 0360 | Loss: 3.2650 | Avg Thinking Steps: 4.39
Ep 0370 | Loss: 2.9194 | Avg Thinking Steps: 5.71
Ep 0380 | Loss: 2.6120 | Avg Thinking Steps: 3.31
Ep 0390 | Loss: 2.0958 | Avg Thinking Steps: 2.45
Ep 0400 | Loss: 1.9232 | Avg Thinking Steps: 2.34
Ep 0410 | Loss: 1.6229 | Avg Thinking Steps: 2.15
Ep 0420 | Loss: 1.4568 | Avg Thinking Steps: 2.05
Ep 0430 | Loss: 1.2279 | Avg Thinking Steps: 1.99
Ep 0440 | Loss: 1.2050 | Avg Thinking Steps: 2.04
Ep 0450 | Loss: 0.9993 | Avg Thinking Steps: 1.96
Ep 0460 | Loss: 0.9242 | Avg Thinking Steps: 1.96
Ep 0470 | Loss: 0.9740 | Avg Thinking Steps: 1.91
Ep 0480 | Loss: 0.7950 | Avg Thinking Steps: 1.83
Ep 0490 | Loss: 0.8090 | Avg Thinking Steps: 1.80
Ep 0500 | Loss: 0.6127 | Avg Thinking Steps: 1.75
Ep 0510 | Loss: 0.5819 | Avg Thinking Steps: 1.72
Ep 0520 | Loss: 0.5871 | Avg Thinking Steps: 1.66
Ep 0530 | Loss: 0.4917 | Avg Thinking Steps: 1.65
Ep 0540 | Loss: 0.7042 | Avg Thinking Steps: 1.69
Ep 0550 | Loss: 0.6083 | Avg Thinking Steps: 1.65
Ep 0560 | Loss: 0.4146 | Avg Thinking Steps: 1.68
Ep 0570 | Loss: 0.3583 | Avg Thinking Steps: 1.64
Ep 0580 | Loss: 0.4417 | Avg Thinking Steps: 1.64
Ep 0590 | Loss: 0.3325 | Avg Thinking Steps: 1.59
Ep 0600 | Loss: 0.3257 | Avg Thinking Steps: 1.60
Ep 0610 | Loss: 0.2937 | Avg Thinking Steps: 1.59
Ep 0620 | Loss: 0.3993 | Avg Thinking Steps: 1.66
Ep 0630 | Loss: 0.2867 | Avg Thinking Steps: 1.65
Ep 0640 | Loss: 0.3931 | Avg Thinking Steps: 1.65
Ep 0650 | Loss: 0.4354 | Avg Thinking Steps: 1.64
Ep 0660 | Loss: 0.2708 | Avg Thinking Steps: 1.58
Ep 0670 | Loss: 0.2348 | Avg Thinking Steps: 1.64
Ep 0680 | Loss: 0.2755 | Avg Thinking Steps: 1.64
Ep 0690 | Loss: 0.3334 | Avg Thinking Steps: 1.63
Ep 0700 | Loss: 0.2855 | Avg Thinking Steps: 1.65
Ep 0710 | Loss: 0.2695 | Avg Thinking Steps: 1.65
Ep 0720 | Loss: 0.3310 | Avg Thinking Steps: 1.62
Ep 0730 | Loss: 0.4050 | Avg Thinking Steps: 1.69
Ep 0740 | Loss: 0.3692 | Avg Thinking Steps: 1.65
Ep 0750 | Loss: 0.3997 | Avg Thinking Steps: 1.66
Ep 0760 | Loss: 0.3542 | Avg Thinking Steps: 1.66
Ep 0770 | Loss: 0.1994 | Avg Thinking Steps: 1.65
Ep 0780 | Loss: 0.2408 | Avg Thinking Steps: 1.66
Ep 0790 | Loss: 0.2049 | Avg Thinking Steps: 1.69
Ep 0800 | Loss: 0.2804 | Avg Thinking Steps: 1.63
Ep 0810 | Loss: 0.3767 | Avg Thinking Steps: 1.66
Ep 0820 | Loss: 0.2599 | Avg Thinking Steps: 1.61
Ep 0830 | Loss: 0.2097 | Avg Thinking Steps: 1.65
Ep 0840 | Loss: 0.2101 | Avg Thinking Steps: 1.63
Ep 0850 | Loss: 0.3465 | Avg Thinking Steps: 1.70
Ep 0860 | Loss: 0.3985 | Avg Thinking Steps: 1.64
Ep 0870 | Loss: 0.2891 | Avg Thinking Steps: 1.70
Ep 0880 | Loss: 0.2988 | Avg Thinking Steps: 1.65
Ep 0890 | Loss: 0.1822 | Avg Thinking Steps: 1.62
Ep 0900 | Loss: 0.2342 | Avg Thinking Steps: 1.68
Ep 0910 | Loss: 0.2413 | Avg Thinking Steps: 1.69
Ep 0920 | Loss: 0.1790 | Avg Thinking Steps: 1.67
Ep 0930 | Loss: 0.1964 | Avg Thinking Steps: 1.72
Ep 0940 | Loss: 0.2228 | Avg Thinking Steps: 1.68
Ep 0950 | Loss: 0.2219 | Avg Thinking Steps: 1.70
Ep 0960 | Loss: 0.2483 | Avg Thinking Steps: 1.67
Ep 0970 | Loss: 0.3355 | Avg Thinking Steps: 1.68
Ep 0980 | Loss: 0.4626 | Avg Thinking Steps: 1.62
Ep 0990 | Loss: 0.2264 | Avg Thinking Steps: 1.61
Ep 1000 | Loss: 0.2454 | Avg Thinking Steps: 1.74
Ep 1010 | Loss: 0.2777 | Avg Thinking Steps: 1.68
Ep 1020 | Loss: 0.2569 | Avg Thinking Steps: 1.68
Ep 1030 | Loss: 0.1668 | Avg Thinking Steps: 1.65
Ep 1040 | Loss: 0.2132 | Avg Thinking Steps: 1.65
Ep 1050 | Loss: 0.2429 | Avg Thinking Steps: 1.64
Ep 1060 | Loss: 0.2914 | Avg Thinking Steps: 1.63
Ep 1070 | Loss: 0.2914 | Avg Thinking Steps: 1.67
Ep 1080 | Loss: 0.2113 | Avg Thinking Steps: 1.67
Ep 1090 | Loss: 0.3034 | Avg Thinking Steps: 1.68
Ep 1100 | Loss: 0.2297 | Avg Thinking Steps: 1.67
Ep 1110 | Loss: 0.1595 | Avg Thinking Steps: 1.66
Ep 1120 | Loss: 0.2258 | Avg Thinking Steps: 1.67
Ep 1130 | Loss: 0.1877 | Avg Thinking Steps: 1.68
Ep 1140 | Loss: 0.2529 | Avg Thinking Steps: 1.65
Ep 1150 | Loss: 0.1926 | Avg Thinking Steps: 1.63
Ep 1160 | Loss: 0.2019 | Avg Thinking Steps: 1.73
Ep 1170 | Loss: 0.1999 | Avg Thinking Steps: 1.67
Ep 1180 | Loss: 0.2272 | Avg Thinking Steps: 1.74
Ep 1190 | Loss: 0.2374 | Avg Thinking Steps: 1.64
Ep 1200 | Loss: 0.1871 | Avg Thinking Steps: 1.70
Ep 1210 | Loss: 0.2294 | Avg Thinking Steps: 1.65
Ep 1220 | Loss: 0.2855 | Avg Thinking Steps: 1.70
Ep 1230 | Loss: 0.2463 | Avg Thinking Steps: 1.68
Ep 1240 | Loss: 0.2427 | Avg Thinking Steps: 1.67
Ep 1250 | Loss: 0.4934 | Avg Thinking Steps: 1.65
Ep 1260 | Loss: 0.3376 | Avg Thinking Steps: 1.67
Ep 1270 | Loss: 0.1882 | Avg Thinking Steps: 1.60
Ep 1280 | Loss: 0.3511 | Avg Thinking Steps: 1.63
Ep 1290 | Loss: 0.2084 | Avg Thinking Steps: 1.70
Ep 1300 | Loss: 0.2377 | Avg Thinking Steps: 1.68
Ep 1310 | Loss: 0.3245 | Avg Thinking Steps: 1.77
Ep 1320 | Loss: 0.2089 | Avg Thinking Steps: 1.68
Ep 1330 | Loss: 0.2111 | Avg Thinking Steps: 1.60
Ep 1340 | Loss: 0.1986 | Avg Thinking Steps: 1.73
Ep 1350 | Loss: 0.1743 | Avg Thinking Steps: 1.70
Ep 1360 | Loss: 0.1939 | Avg Thinking Steps: 1.74
Ep 1370 | Loss: 0.1361 | Avg Thinking Steps: 1.65
Ep 1380 | Loss: 0.1696 | Avg Thinking Steps: 1.69
Ep 1390 | Loss: 0.1752 | Avg Thinking Steps: 1.72
Ep 1400 | Loss: 0.2903 | Avg Thinking Steps: 1.71
Ep 1410 | Loss: 0.2634 | Avg Thinking Steps: 1.73
Ep 1420 | Loss: 0.1531 | Avg Thinking Steps: 1.73
Ep 1430 | Loss: 0.2451 | Avg Thinking Steps: 1.70
Ep 1440 | Loss: 0.1964 | Avg Thinking Steps: 1.74
Ep 1450 | Loss: 0.2197 | Avg Thinking Steps: 1.81
Ep 1460 | Loss: 0.1554 | Avg Thinking Steps: 1.76
Ep 1470 | Loss: 0.2236 | Avg Thinking Steps: 1.73
Ep 1480 | Loss: 0.2062 | Avg Thinking Steps: 1.73
Ep 1490 | Loss: 0.1330 | Avg Thinking Steps: 1.75
Ep 1500 | Loss: 0.1822 | Avg Thinking Steps: 1.70
Ep 1510 | Loss: 0.1181 | Avg Thinking Steps: 1.71
Ep 1520 | Loss: 0.1781 | Avg Thinking Steps: 1.75
Ep 1530 | Loss: 0.1300 | Avg Thinking Steps: 1.76
Ep 1540 | Loss: 0.1127 | Avg Thinking Steps: 1.69
Ep 1550 | Loss: 0.1771 | Avg Thinking Steps: 1.72
Ep 1560 | Loss: 0.2033 | Avg Thinking Steps: 1.67
Ep 1570 | Loss: 0.1977 | Avg Thinking Steps: 1.65
Ep 1580 | Loss: 0.1363 | Avg Thinking Steps: 1.73
Ep 1590 | Loss: 0.2438 | Avg Thinking Steps: 1.70
Ep 1600 | Loss: 0.1876 | Avg Thinking Steps: 1.72
Ep 1610 | Loss: 0.1336 | Avg Thinking Steps: 1.70
Ep 1620 | Loss: 0.2065 | Avg Thinking Steps: 1.70
Ep 1630 | Loss: 0.1571 | Avg Thinking Steps: 1.70
Ep 1640 | Loss: 0.1180 | Avg Thinking Steps: 1.74
Ep 1650 | Loss: 0.1826 | Avg Thinking Steps: 1.70
Ep 1660 | Loss: 0.1874 | Avg Thinking Steps: 1.71
Ep 1670 | Loss: 0.2426 | Avg Thinking Steps: 1.69
Ep 1680 | Loss: 0.1402 | Avg Thinking Steps: 1.75
Ep 1690 | Loss: 0.2651 | Avg Thinking Steps: 1.72
Ep 1700 | Loss: 0.2038 | Avg Thinking Steps: 1.73
Ep 1710 | Loss: 0.2910 | Avg Thinking Steps: 1.67
Ep 1720 | Loss: 0.1847 | Avg Thinking Steps: 1.69
Ep 1730 | Loss: 0.2253 | Avg Thinking Steps: 1.69
Ep 1740 | Loss: 0.2470 | Avg Thinking Steps: 1.73
Ep 1750 | Loss: 0.1911 | Avg Thinking Steps: 1.76
Ep 1760 | Loss: 0.1810 | Avg Thinking Steps: 1.73
Ep 1770 | Loss: 0.1865 | Avg Thinking Steps: 1.77
Ep 1780 | Loss: 0.1690 | Avg Thinking Steps: 1.73
Ep 1790 | Loss: 0.1531 | Avg Thinking Steps: 1.74
Ep 1800 | Loss: 0.1692 | Avg Thinking Steps: 1.74
Ep 1810 | Loss: 0.2073 | Avg Thinking Steps: 1.82
Ep 1820 | Loss: 0.2719 | Avg Thinking Steps: 1.75
Ep 1830 | Loss: 0.2058 | Avg Thinking Steps: 1.74
Ep 1840 | Loss: 0.3064 | Avg Thinking Steps: 1.72
Ep 1850 | Loss: 0.1038 | Avg Thinking Steps: 1.73
Ep 1860 | Loss: 0.4624 | Avg Thinking Steps: 1.76
Ep 1870 | Loss: 0.2375 | Avg Thinking Steps: 1.81
Ep 1880 | Loss: 0.1649 | Avg Thinking Steps: 1.73
Ep 1890 | Loss: 0.2292 | Avg Thinking Steps: 1.74
Ep 1900 | Loss: 0.2384 | Avg Thinking Steps: 1.67
Ep 1910 | Loss: 0.1491 | Avg Thinking Steps: 1.76
Ep 1920 | Loss: 0.2017 | Avg Thinking Steps: 1.72
Ep 1930 | Loss: 0.1974 | Avg Thinking Steps: 1.68
Ep 1940 | Loss: 0.2005 | Avg Thinking Steps: 1.68
Ep 1950 | Loss: 0.1914 | Avg Thinking Steps: 1.69
Ep 1960 | Loss: 0.1434 | Avg Thinking Steps: 1.70
Ep 1970 | Loss: 0.2100 | Avg Thinking Steps: 1.71
Ep 1980 | Loss: 0.1404 | Avg Thinking Steps: 1.74
Ep 1990 | Loss: 0.2541 | Avg Thinking Steps: 1.74
Ep 2000 | Loss: 0.1638 | Avg Thinking Steps: 1.74
Ep 2010 | Loss: 0.1547 | Avg Thinking Steps: 1.71
Ep 2020 | Loss: 0.1236 | Avg Thinking Steps: 1.72
Ep 2030 | Loss: 0.1538 | Avg Thinking Steps: 1.73
Ep 2040 | Loss: 0.2555 | Avg Thinking Steps: 1.78
Ep 2050 | Loss: 0.1138 | Avg Thinking Steps: 1.70
Ep 2060 | Loss: 0.1403 | Avg Thinking Steps: 1.72
Ep 2070 | Loss: 0.1990 | Avg Thinking Steps: 1.65
Ep 2080 | Loss: 0.1556 | Avg Thinking Steps: 1.70
Ep 2090 | Loss: 0.1690 | Avg Thinking Steps: 1.71
Ep 2100 | Loss: 0.2132 | Avg Thinking Steps: 1.78
Ep 2110 | Loss: 0.1985 | Avg Thinking Steps: 1.72
Ep 2120 | Loss: 0.1141 | Avg Thinking Steps: 1.72
Ep 2130 | Loss: 0.1576 | Avg Thinking Steps: 1.72
Ep 2140 | Loss: 0.1711 | Avg Thinking Steps: 1.76
Ep 2150 | Loss: 0.2383 | Avg Thinking Steps: 1.70
Ep 2160 | Loss: 0.1276 | Avg Thinking Steps: 1.67
Ep 2170 | Loss: 0.1790 | Avg Thinking Steps: 1.74
Ep 2180 | Loss: 0.1142 | Avg Thinking Steps: 1.73
Ep 2190 | Loss: 0.0821 | Avg Thinking Steps: 1.76
Ep 2200 | Loss: 0.2312 | Avg Thinking Steps: 1.74
Ep 2210 | Loss: 0.0984 | Avg Thinking Steps: 1.73
Ep 2220 | Loss: 0.1634 | Avg Thinking Steps: 1.75
Ep 2230 | Loss: 0.1379 | Avg Thinking Steps: 1.74
Ep 2240 | Loss: 0.1640 | Avg Thinking Steps: 1.77
Ep 2250 | Loss: 0.1514 | Avg Thinking Steps: 1.72
Ep 2260 | Loss: 0.1218 | Avg Thinking Steps: 1.70
Ep 2270 | Loss: 0.1525 | Avg Thinking Steps: 1.70
Ep 2280 | Loss: 0.1374 | Avg Thinking Steps: 1.71
Ep 2290 | Loss: 0.1903 | Avg Thinking Steps: 1.68
Ep 2300 | Loss: 0.1416 | Avg Thinking Steps: 1.67
Ep 2310 | Loss: 0.2223 | Avg Thinking Steps: 1.73
Ep 2320 | Loss: 0.1097 | Avg Thinking Steps: 1.67
Ep 2330 | Loss: 0.1694 | Avg Thinking Steps: 1.71
Ep 2340 | Loss: 0.1006 | Avg Thinking Steps: 1.74
Ep 2350 | Loss: 0.2418 | Avg Thinking Steps: 1.70
Ep 2360 | Loss: 0.1443 | Avg Thinking Steps: 1.72
Ep 2370 | Loss: 0.1162 | Avg Thinking Steps: 1.69
Ep 2380 | Loss: 0.1944 | Avg Thinking Steps: 1.69
Ep 2390 | Loss: 0.1429 | Avg Thinking Steps: 1.74
Ep 2400 | Loss: 0.1428 | Avg Thinking Steps: 1.74
Ep 2410 | Loss: 0.1738 | Avg Thinking Steps: 1.69
Ep 2420 | Loss: 0.2143 | Avg Thinking Steps: 1.74
Ep 2430 | Loss: 0.1555 | Avg Thinking Steps: 1.73
Ep 2440 | Loss: 0.1329 | Avg Thinking Steps: 1.72
Ep 2450 | Loss: 0.0938 | Avg Thinking Steps: 1.74
Ep 2460 | Loss: 0.1143 | Avg Thinking Steps: 1.69
Ep 2470 | Loss: 0.1055 | Avg Thinking Steps: 1.77
Ep 2480 | Loss: 0.1409 | Avg Thinking Steps: 1.70
Ep 2490 | Loss: 0.1163 | Avg Thinking Steps: 1.75
Ep 2500 | Loss: 0.1647 | Avg Thinking Steps: 1.74
Ep 2510 | Loss: 0.1783 | Avg Thinking Steps: 1.74
Ep 2520 | Loss: 0.2229 | Avg Thinking Steps: 1.81
Ep 2530 | Loss: 0.2161 | Avg Thinking Steps: 1.74
Ep 2540 | Loss: 0.2113 | Avg Thinking Steps: 1.72
Ep 2550 | Loss: 0.2907 | Avg Thinking Steps: 1.69
Ep 2560 | Loss: 0.1778 | Avg Thinking Steps: 1.64
Ep 2570 | Loss: 0.1948 | Avg Thinking Steps: 1.71
Ep 2580 | Loss: 0.1680 | Avg Thinking Steps: 1.69
Ep 2590 | Loss: 0.1724 | Avg Thinking Steps: 1.65
Ep 2600 | Loss: 0.1663 | Avg Thinking Steps: 1.64
Ep 2610 | Loss: 0.1326 | Avg Thinking Steps: 1.61
Ep 2620 | Loss: 0.1617 | Avg Thinking Steps: 1.65
Ep 2630 | Loss: 0.1606 | Avg Thinking Steps: 1.69
Ep 2640 | Loss: 0.0948 | Avg Thinking Steps: 1.64
Ep 2650 | Loss: 0.1541 | Avg Thinking Steps: 1.64
Ep 2660 | Loss: 0.1741 | Avg Thinking Steps: 1.66
Ep 2670 | Loss: 0.1255 | Avg Thinking Steps: 1.62
Ep 2680 | Loss: 0.1738 | Avg Thinking Steps: 1.59
Ep 2690 | Loss: 0.1786 | Avg Thinking Steps: 1.60
Ep 2700 | Loss: 0.1069 | Avg Thinking Steps: 1.59
Ep 2710 | Loss: 0.1352 | Avg Thinking Steps: 1.67
Ep 2720 | Loss: 0.1721 | Avg Thinking Steps: 1.69
Ep 2730 | Loss: 0.0908 | Avg Thinking Steps: 1.57
Ep 2740 | Loss: 0.1304 | Avg Thinking Steps: 1.60
Ep 2750 | Loss: 0.1408 | Avg Thinking Steps: 1.69
Ep 2760 | Loss: 0.1542 | Avg Thinking Steps: 1.64
Ep 2770 | Loss: 0.1259 | Avg Thinking Steps: 1.63
Ep 2780 | Loss: 0.1360 | Avg Thinking Steps: 1.62
Ep 2790 | Loss: 0.1690 | Avg Thinking Steps: 1.63
Ep 2800 | Loss: 0.1428 | Avg Thinking Steps: 1.69
Ep 2810 | Loss: 0.1602 | Avg Thinking Steps: 1.60
Ep 2820 | Loss: 0.0953 | Avg Thinking Steps: 1.61
Ep 2830 | Loss: 0.2412 | Avg Thinking Steps: 1.66
Ep 2840 | Loss: 0.1301 | Avg Thinking Steps: 1.63
Ep 2850 | Loss: 0.2176 | Avg Thinking Steps: 1.62
Ep 2860 | Loss: 0.1691 | Avg Thinking Steps: 1.63
Ep 2870 | Loss: 0.1807 | Avg Thinking Steps: 1.58
Ep 2880 | Loss: 0.2175 | Avg Thinking Steps: 1.63
Ep 2890 | Loss: 0.1712 | Avg Thinking Steps: 1.62
Ep 2900 | Loss: 0.1751 | Avg Thinking Steps: 1.58
Ep 2910 | Loss: 0.1821 | Avg Thinking Steps: 1.64
Ep 2920 | Loss: 0.1587 | Avg Thinking Steps: 1.66
Ep 2930 | Loss: 0.2035 | Avg Thinking Steps: 1.65
Ep 2940 | Loss: 0.1874 | Avg Thinking Steps: 1.68
Ep 2950 | Loss: 0.1173 | Avg Thinking Steps: 1.64
Ep 2960 | Loss: 0.0980 | Avg Thinking Steps: 1.62
Ep 2970 | Loss: 0.2413 | Avg Thinking Steps: 1.61
Ep 2980 | Loss: 0.1367 | Avg Thinking Steps: 1.67
Ep 2990 | Loss: 0.1698 | Avg Thinking Steps: 1.67

--- Saving Model to crsn_complex_model.pth ---
Saved.
Model saved locally to /home/dan/Desktop/ProjectX/crsn_complex_model.pth

--- Visualizing Learned Symbolic Graph ---
Graph sparse.

<img width="690" height="803" alt="Figure_5" src="https://github.com/user-attachments/assets/3d22f9fc-68e2-4337-8ef5-9a296d86f7b3" />

Generated: That which is like to that which is like to that which is like to that which is like to that which is

------------------------------------------------------------------------------------------------------------------------------------------------

SACRSNv2

Ep 0000 | Loss 1130.275 | Avg steps 7.60
Ep 0020 | Loss 862.329 | Avg steps 4.44
Ep 0040 | Loss 813.315 | Avg steps 3.18
Ep 0060 | Loss 824.550 | Avg steps 2.69
Ep 0080 | Loss 813.131 | Avg steps 2.54
Ep 0100 | Loss 765.027 | Avg steps 2.44
Ep 0120 | Loss 760.687 | Avg steps 2.32
Ep 0140 | Loss 800.243 | Avg steps 2.33
Ep 0160 | Loss 754.261 | Avg steps 2.31
Ep 0180 | Loss 732.511 | Avg steps 2.17
Ep 0200 | Loss 799.604 | Avg steps 2.35
Ep 0220 | Loss 779.110 | Avg steps 2.26
Ep 0240 | Loss 773.682 | Avg steps 2.14
Ep 0260 | Loss 723.674 | Avg steps 2.04
Ep 0280 | Loss 730.683 | Avg steps 2.00
Ep 0300 | Loss 695.840 | Avg steps 2.00
Ep 0320 | Loss 700.684 | Avg steps 1.99
Ep 0340 | Loss 728.172 | Avg steps 1.97
Ep 0360 | Loss 606.033 | Avg steps 1.93
Ep 0380 | Loss 586.759 | Avg steps 1.78
Ep 0400 | Loss 619.481 | Avg steps 1.73
Ep 0420 | Loss 530.730 | Avg steps 1.74
Ep 0440 | Loss 492.575 | Avg steps 1.77
Ep 0460 | Loss 477.985 | Avg steps 1.71
Ep 0480 | Loss 412.097 | Avg steps 1.56
Ep 0500 | Loss 384.170 | Avg steps 1.60
Ep 0520 | Loss 333.306 | Avg steps 1.42
Ep 0540 | Loss 327.590 | Avg steps 1.40
Ep 0560 | Loss 273.014 | Avg steps 1.33
Ep 0580 | Loss 251.833 | Avg steps 1.29

--- Symbolic Graph ---
Graph still sparse.

Generated:
Tr above.
The fant wion is below is below is below is below is below is above.
Th is below is below is above.
Th is below is orthich is below is like i

----------------------------------------------------------------------------------------------------------------------

SACRSNv7

Ep 0000 | Loss: 4.4031 | Steps: 7.92 | LR: 0.001000
Ep 0050 | Loss: 3.0882 | Steps: 4.20 | LR: 0.000999
Ep 0100 | Loss: 3.1524 | Steps: 3.23 | LR: 0.000997
Ep 0150 | Loss: 3.2100 | Steps: 4.90 | LR: 0.000994
Ep 0200 | Loss: 3.0778 | Steps: 3.91 | LR: 0.000989
Ep 0250 | Loss: 2.4788 | Steps: 3.22 | LR: 0.000983
Ep 0300 | Loss: 1.1446 | Steps: 1.96 | LR: 0.000976
Ep 0350 | Loss: 0.7598 | Steps: 1.81 | LR: 0.000967
Ep 0400 | Loss: 0.6822 | Steps: 1.59 | LR: 0.000957
Ep 0450 | Loss: 0.3569 | Steps: 1.42 | LR: 0.000946
Ep 0500 | Loss: 0.3952 | Steps: 1.45 | LR: 0.000933
Ep 0550 | Loss: 0.3968 | Steps: 1.48 | LR: 0.000920
Ep 0600 | Loss: 0.3562 | Steps: 1.53 | LR: 0.000905
Ep 0650 | Loss: 0.2170 | Steps: 1.49 | LR: 0.000889
Ep 0700 | Loss: 0.2440 | Steps: 1.45 | LR: 0.000873
Ep 0750 | Loss: 0.2684 | Steps: 1.42 | LR: 0.000855
Ep 0800 | Loss: 0.3100 | Steps: 1.42 | LR: 0.000836
Ep 0850 | Loss: 0.2111 | Steps: 1.51 | LR: 0.000816
Ep 0900 | Loss: 0.2104 | Steps: 1.41 | LR: 0.000796
Ep 0950 | Loss: 0.2469 | Steps: 1.37 | LR: 0.000774
Ep 1000 | Loss: 0.1579 | Steps: 1.45 | LR: 0.000752
Ep 1050 | Loss: 0.2081 | Steps: 1.41 | LR: 0.000729
Ep 1100 | Loss: 0.2145 | Steps: 1.42 | LR: 0.000706
Ep 1150 | Loss: 0.1718 | Steps: 1.44 | LR: 0.000682
Ep 1200 | Loss: 0.1756 | Steps: 1.46 | LR: 0.000657
Ep 1250 | Loss: 0.1723 | Steps: 1.50 | LR: 0.000633
Ep 1300 | Loss: 0.1478 | Steps: 1.45 | LR: 0.000607
Ep 1350 | Loss: 0.1489 | Steps: 1.51 | LR: 0.000582
Ep 1400 | Loss: 0.1744 | Steps: 1.55 | LR: 0.000556
Ep 1450 | Loss: 0.2148 | Steps: 1.54 | LR: 0.000530
Ep 1500 | Loss: 0.2128 | Steps: 1.57 | LR: 0.000504
Ep 1550 | Loss: 0.1288 | Steps: 1.51 | LR: 0.000479
Ep 1600 | Loss: 0.1732 | Steps: 1.58 | LR: 0.000453
Ep 1650 | Loss: 0.1338 | Steps: 1.52 | LR: 0.000427
Ep 1700 | Loss: 0.1423 | Steps: 1.54 | LR: 0.000402
Ep 1750 | Loss: 0.1516 | Steps: 1.57 | LR: 0.000376
Ep 1800 | Loss: 0.1127 | Steps: 1.63 | LR: 0.000352
Ep 1850 | Loss: 0.1209 | Steps: 1.60 | LR: 0.000327
Ep 1900 | Loss: 0.1000 | Steps: 1.61 | LR: 0.000303
Ep 1950 | Loss: 0.1137 | Steps: 1.58 | LR: 0.000280
Ep 2000 | Loss: 0.0944 | Steps: 1.57 | LR: 0.000257
Ep 2050 | Loss: 0.1191 | Steps: 1.60 | LR: 0.000235
Ep 2100 | Loss: 0.1524 | Steps: 1.62 | LR: 0.000214
Ep 2150 | Loss: 0.1415 | Steps: 1.62 | LR: 0.000193
Ep 2200 | Loss: 0.1411 | Steps: 1.63 | LR: 0.000173
Ep 2250 | Loss: 0.1317 | Steps: 1.61 | LR: 0.000155
Ep 2300 | Loss: 0.1238 | Steps: 1.63 | LR: 0.000137
Ep 2350 | Loss: 0.1046 | Steps: 1.65 | LR: 0.000120
Ep 2400 | Loss: 0.1136 | Steps: 1.62 | LR: 0.000104
Ep 2450 | Loss: 0.1112 | Steps: 1.60 | LR: 0.000090
Ep 2500 | Loss: 0.1098 | Steps: 1.64 | LR: 0.000076
Ep 2550 | Loss: 0.0754 | Steps: 1.64 | LR: 0.000064
Ep 2600 | Loss: 0.0871 | Steps: 1.62 | LR: 0.000053
Ep 2650 | Loss: 0.0793 | Steps: 1.61 | LR: 0.000043
Ep 2700 | Loss: 0.0833 | Steps: 1.59 | LR: 0.000034
Ep 2750 | Loss: 0.0950 | Steps: 1.59 | LR: 0.000027
Ep 2800 | Loss: 0.0824 | Steps: 1.59 | LR: 0.000021
Ep 2850 | Loss: 0.0684 | Steps: 1.59 | LR: 0.000016
Ep 2900 | Loss: 0.0713 | Steps: 1.60 | LR: 0.000013
Ep 2950 | Loss: 0.0681 | Steps: 1.58 | LR: 0.000011

--- Saving Model to crsn_ultimate_final.pth ---
Saved.

--- Visualizing Learned Symbolic Graph ---
Active Symbols: 64 / 64

<img width="1199" height="804" alt="Figure_6" src="https://github.com/user-attachments/assets/a0abc07e-3b53-4c58-81cf-5f3c2059bca0" />


Generated: That which is above.
The in the er of all pe conve in the in thout fals like to that which is below is like to the if it be conve if it be conve in the in ther of all pe.ue, without fals like to that w


--- Extracting Explicit Logic Rules ---

FROM   | TO     | COUNT  | AVG THINKING STEPS
---------------------------------------------
S_60   -> S_60   | 188    | 1.41
S_60   -> S_51   | 35     | 1.91
S_51   -> S_60   | 34     | 1.47
S_51   -> S_51   | 15     | 1.87

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

SACRSNv10

Ep 0000 | Loss: 4.3712 | Steps: 7.81 | LR: 0.001000
Ep 0050 | Loss: 3.3877 | Steps: 3.35 | LR: 0.000994
Ep 0100 | Loss: 3.0533 | Steps: 3.28 | LR: 0.000975
Ep 0150 | Loss: 3.1049 | Steps: 4.04 | LR: 0.000945
Ep 0200 | Loss: 3.3860 | Steps: 3.76 | LR: 0.000905
Ep 0250 | Loss: 3.3857 | Steps: 3.28 | LR: 0.000854
Ep 0300 | Loss: 3.1808 | Steps: 3.68 | LR: 0.000795
Ep 0350 | Loss: 3.1181 | Steps: 3.77 | LR: 0.000728
Ep 0400 | Loss: 3.0057 | Steps: 3.67 | LR: 0.000656
Ep 0450 | Loss: 2.9156 | Steps: 3.09 | LR: 0.000581
Ep 0500 | Loss: 2.7262 | Steps: 3.65 | LR: 0.000503
Ep 0550 | Loss: 2.5114 | Steps: 3.29 | LR: 0.000426
Ep 0600 | Loss: 1.5972 | Steps: 2.24 | LR: 0.000351
Ep 0650 | Loss: 1.3326 | Steps: 2.20 | LR: 0.000279
Ep 0700 | Loss: 1.0680 | Steps: 2.02 | LR: 0.000213
Ep 0750 | Loss: 0.8797 | Steps: 1.92 | LR: 0.000154
Ep 0800 | Loss: 0.7921 | Steps: 1.89 | LR: 0.000104
Ep 0850 | Loss: 0.6617 | Steps: 1.80 | LR: 0.000063
Ep 0900 | Loss: 0.6557 | Steps: 1.79 | LR: 0.000034
Ep 0950 | Loss: 0.6815 | Steps: 1.76 | LR: 0.000016

--- Saving Model to crsn_ultimate_final.pth ---
Saved.

--- Visualizing Learned Symbolic Graph ---
Active Symbols: 64 / 64

Generated: Tre is above.
The is like is below is fo thout is force on it below ents fthove is force of it belowenk ittionerted is like to ther of all pe the conhither of above.
Thich is force of fald 
and is belo

Ep 0000 | Loss: 4.3712 | Steps: 7.81 | LR: 0.001000
Ep 0050 | Loss: 3.2584 | Steps: 3.34 | LR: 0.000997
Ep 0100 | Loss: 2.8783 | Steps: 2.10 | LR: 0.000989
Ep 0150 | Loss: 1.3445 | Steps: 1.87 | LR: 0.000975
Ep 0200 | Loss: 0.8178 | Steps: 1.65 | LR: 0.000957
Ep 0250 | Loss: 0.5996 | Steps: 1.35 | LR: 0.000933
Ep 0300 | Loss: 0.5065 | Steps: 1.49 | LR: 0.000905
Ep 0350 | Loss: 0.3000 | Steps: 1.49 | LR: 0.000872
Ep 0400 | Loss: 0.3460 | Steps: 1.52 | LR: 0.000835
Ep 0450 | Loss: 0.2393 | Steps: 1.66 | LR: 0.000795
Ep 0500 | Loss: 0.3101 | Steps: 1.57 | LR: 0.000752
Ep 0550 | Loss: 0.2589 | Steps: 1.61 | LR: 0.000705
Ep 0600 | Loss: 0.2873 | Steps: 1.68 | LR: 0.000657
Ep 0650 | Loss: 0.2761 | Steps: 1.66 | LR: 0.000607
Ep 0700 | Loss: 0.1524 | Steps: 1.73 | LR: 0.000556
Ep 0750 | Loss: 0.2174 | Steps: 1.73 | LR: 0.000504
Ep 0800 | Loss: 0.1421 | Steps: 1.77 | LR: 0.000452
Ep 0850 | Loss: 0.1340 | Steps: 1.76 | LR: 0.000401
Ep 0900 | Loss: 0.1596 | Steps: 1.73 | LR: 0.000351
Ep 0950 | Loss: 0.1566 | Steps: 1.77 | LR: 0.000303
Ep 1000 | Loss: 0.1576 | Steps: 1.81 | LR: 0.000257
Ep 1050 | Loss: 0.1194 | Steps: 1.80 | LR: 0.000213
Ep 1100 | Loss: 0.1325 | Steps: 1.77 | LR: 0.000173
Ep 1150 | Loss: 0.1258 | Steps: 1.79 | LR: 0.000136
Ep 1200 | Loss: 0.1311 | Steps: 1.75 | LR: 0.000104
Ep 1250 | Loss: 0.1715 | Steps: 1.78 | LR: 0.000076
Ep 1300 | Loss: 0.0921 | Steps: 1.75 | LR: 0.000052
Ep 1350 | Loss: 0.0789 | Steps: 1.77 | LR: 0.000034
Ep 1400 | Loss: 0.0908 | Steps: 1.74 | LR: 0.000021
Ep 1450 | Loss: 0.0952 | Steps: 1.74 | LR: 0.000013

--- Saving Model to crsn_ultimate_final.pth ---
Saved.

--- Visualizing Learned Symbolic Graph ---
Active Symbols: 64 / 64

Generated: Trth. 
Ther of all pelow be conve is like to that which is he wirue. 
The ch is hicf is entirtain aathout fals lintirue. 
Therth. 
Th is below is like to tirtain anto earth.d ind most true. 
Therue.
Th


--- Extracting Explicit Logic Rules ---

FROM   | TO     | COUNT  | AVG THINKING STEPS
---------------------------------------------
S_27   -> S_27   | 100    | 1.77
S_45   -> S_45   | 81     | 1.85
S_45   -> S_27   | 46     | 1.80
S_27   -> S_45   | 45     | 1.62

----------------------------------------------------------------------------------------------------------------------------

SACRSNv11

Ep 0000 | Loss: 4.3681 | Steps: 7.67 | LR: 0.001000
Ep 0050 | Loss: 3.2900 | Steps: 4.79 | LR: 0.000997
Ep 0100 | Loss: 3.1286 | Steps: 4.21 | LR: 0.000989
Ep 0150 | Loss: 3.1984 | Steps: 3.91 | LR: 0.000975
Ep 0200 | Loss: 3.1793 | Steps: 5.16 | LR: 0.000957
Ep 0250 | Loss: 3.0110 | Steps: 4.60 | LR: 0.000933
Ep 0300 | Loss: 2.2537 | Steps: 3.72 | LR: 0.000905
Ep 0350 | Loss: 1.3957 | Steps: 2.52 | LR: 0.000872
Ep 0400 | Loss: 0.9289 | Steps: 1.97 | LR: 0.000835
Ep 0450 | Loss: 0.6270 | Steps: 1.96 | LR: 0.000795
Ep 0500 | Loss: 0.7609 | Steps: 1.75 | LR: 0.000752
Ep 0550 | Loss: 0.4330 | Steps: 1.72 | LR: 0.000705
Ep 0600 | Loss: 0.3764 | Steps: 1.73 | LR: 0.000657
Ep 0650 | Loss: 0.3396 | Steps: 1.70 | LR: 0.000607
Ep 0700 | Loss: 0.3139 | Steps: 1.69 | LR: 0.000556
Ep 0750 | Loss: 0.2809 | Steps: 1.64 | LR: 0.000504
Ep 0800 | Loss: 0.2822 | Steps: 1.64 | LR: 0.000452
Ep 0850 | Loss: 0.2071 | Steps: 1.64 | LR: 0.000401
Ep 0900 | Loss: 0.2578 | Steps: 1.61 | LR: 0.000351
Ep 0950 | Loss: 0.2563 | Steps: 1.64 | LR: 0.000303
Ep 1000 | Loss: 0.1787 | Steps: 1.71 | LR: 0.000257
Ep 1050 | Loss: 0.1865 | Steps: 1.69 | LR: 0.000213
Ep 1100 | Loss: 0.2009 | Steps: 1.72 | LR: 0.000173
Ep 1150 | Loss: 0.1751 | Steps: 1.71 | LR: 0.000136
Ep 1200 | Loss: 0.1300 | Steps: 1.70 | LR: 0.000104
Ep 1250 | Loss: 0.2245 | Steps: 1.71 | LR: 0.000076
Ep 1300 | Loss: 0.1450 | Steps: 1.71 | LR: 0.000052
Ep 1350 | Loss: 0.1377 | Steps: 1.72 | LR: 0.000034
Ep 1400 | Loss: 0.1154 | Steps: 1.71 | LR: 0.000021
Ep 1450 | Loss: 0.1404 | Steps: 1.71 | LR: 0.000013

--- Saving Model to crsn_ultimate_final.pth ---
Saved.

--- Visualizing Learned Symbolic Graph ---
Active Symbols: 64 / 64

<img width="950" height="966" alt="download" src="https://github.com/user-attachments/assets/400e684a-ef22-42aa-9120-6ecbf89886ba" />


Generated: Tr tre.
Its force or is like to that which is like to that which is below is above.
The fals tire to that which is below is below is like to that which is below is like to that which is like to that wh


--- Extracting Explicit Logic Rules ---

FROM   | TO     | COUNT  | AVG THINKING STEPS
---------------------------------------------
S_27   -> S_45   | 55     | 1.56
S_37   -> S_27   | 52     | 1.44
S_27   -> S_27   | 40     | 1.40
S_45   -> S_37   | 34     | 1.97
S_45   -> S_27   | 31     | 1.81
S_27   -> S_37   | 28     | 1.43
S_45   -> S_45   | 17     | 1.76
S_37   -> S_45   | 10     | 3.40
S_37   -> S_37   | 3      | 2.00

------------------------------------------------------------------------------------------------------------------------------------------------------

SACRSNv26 Short run didn't fininsh

Ep 0000 | Loss: 7.6792 | Steps: 7.89 | Usage(PPX): 1.0 | LR: 0.001000
Ep 0050 | Loss: 3.3161 | Steps: 4.14 | Usage(PPX): 1.0 | LR: 0.000999
Ep 0100 | Loss: 3.2199 | Steps: 3.53 | Usage(PPX): 1.0 | LR: 0.000997
Ep 0150 | Loss: 2.9605 | Steps: 2.11 | Usage(PPX): 1.0 | LR: 0.000994
Ep 0200 | Loss: 1.2878 | Steps: 1.62 | Usage(PPX): 1.0 | LR: 0.000989
Ep 0250 | Loss: 0.8349 | Steps: 1.47 | Usage(PPX): 1.0 | LR: 0.000983
Ep 0300 | Loss: 0.7302 | Steps: 1.47 | Usage(PPX): 1.0 | LR: 0.000976
Ep 0350 | Loss: 0.6946 | Steps: 1.55 | Usage(PPX): 1.0 | LR: 0.000967
Ep 0400 | Loss: 0.8377 | Steps: 1.72 | Usage(PPX): 1.0 | LR: 0.000957
Ep 0450 | Loss: 0.5889 | Steps: 1.58 | Usage(PPX): 1.0 | LR: 0.000946
Ep 0500 | Loss: 0.4703 | Steps: 1.54 | Usage(PPX): 1.0 | LR: 0.000933
Ep 0550 | Loss: 0.4675 | Steps: 1.50 | Usage(PPX): 1.0 | LR: 0.000920
Ep 0600 | Loss: 0.3854 | Steps: 1.64 | Usage(PPX): 1.0 | LR: 0.000905
Ep 0650 | Loss: 0.4167 | Steps: 1.61 | Usage(PPX): 1.0 | LR: 0.000889
Ep 0700 | Loss: 0.4584 | Steps: 1.68 | Usage(PPX): 1.0 | LR: 0.000873

Interrupted.

 Saving Model to crsn_omni_model.pth 
Saved.

 Generating Diagnostics & Images 
Mapping Symbols to Text
Saved 1_semantic_topology.png
Running Inference Scan
Generated: Tr fals force of is ce on true.
The o ee, certain als force on in the to earth. int is lh above.
The o trth.
The o eals force of all pe to thich is entirhood, cele world is habove.
The o eld that whi

 Extracting Explicit Logic Rules 
FROM | TO | COUNT | AVG STEPS

S_42  S_42 | 268 | 1.62
S_23  S_23 | 3 | 2.00

 :crescent_moon: Dream Mode (Pure Symbolic Walk) 
Dream Output: T tol t

 :rotating_light: Anomaly Detection Test 
Input: True without falsehood certain

<img width="1000" height="400" alt="5_anomaly_detection" src="https://github.com/user-attachments/assets/d4046354-c67b-4e01-99c7-547016c10598" />
<img width="800" height="800" alt="4_phase_plot" src="https://github.com/user-attachments/assets/546e7ba7-6c9a-4a1f-a059-da09cef1ac73" />
<img width="1200" height="400" alt="3_act_profile" src="https://github.com/user-attachments/assets/afccc2df-f2a2-49f4-8121-095093f90730" />
<img width="1200" height="400" alt="2_stack_mri" src="https://github.com/user-attachments/assets/b2119f5a-f88b-4a39-b992-00b24908f63e" />
<img width="2100" height="2100" alt="1_semantic_topology" src="https://github.com/user-attachments/assets/09f5051c-75de-4bfd-a4f1-a2ecff3c7874" />

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

SACRSNv26-1


----------------------------------------------------------------------------------------------------------------------------------------------------------------------------

SACRSNv27

Ep 0000 | Loss: 4.0833 | Steps: 7.77 | Usage(PPX): 1.0 | LR: 0.000109
Ep 0050 | Loss: 4.0094 | Steps: 7.24 | Usage(PPX): 1.0 | LR: 0.000559
Ep 0100 | Loss: 4.0575 | Steps: 6.78 | Usage(PPX): 1.0 | LR: 0.001000
Ep 0150 | Loss: 4.8585 | Steps: 7.69 | Usage(PPX): 1.0 | LR: 0.000999
Ep 0200 | Loss: 5.2449 | Steps: 7.55 | Usage(PPX): 1.0 | LR: 0.000997
Ep 0250 | Loss: 4.3603 | Steps: 7.34 | Usage(PPX): 1.0 | LR: 0.000993
Ep 0300 | Loss: 2.5004 | Steps: 4.50 | Usage(PPX): 1.0 | LR: 0.000988
Ep 0350 | Loss: 1.7094 | Steps: 2.10 | Usage(PPX): 1.0 | LR: 0.000982
Ep 0400 | Loss: 1.1630 | Steps: 1.87 | Usage(PPX): 1.0 | LR: 0.000974
Ep 0450 | Loss: 1.1030 | Steps: 1.72 | Usage(PPX): 1.0 | LR: 0.000965
Ep 0500 | Loss: 1.2623 | Steps: 1.64 | Usage(PPX): 1.0 | LR: 0.000954
Ep 0550 | Loss: 1.1349 | Steps: 1.81 | Usage(PPX): 1.0 | LR: 0.000942
Ep 0600 | Loss: 1.2297 | Steps: 1.74 | Usage(PPX): 1.0 | LR: 0.000929
Ep 0650 | Loss: 0.9114 | Steps: 1.66 | Usage(PPX): 1.0 | LR: 0.000914
Ep 0700 | Loss: 0.9918 | Steps: 1.78 | Usage(PPX): 1.0 | LR: 0.000899
Ep 0750 | Loss: 1.1933 | Steps: 1.82 | Usage(PPX): 1.0 | LR: 0.000882
Ep 0800 | Loss: 0.9970 | Steps: 1.85 | Usage(PPX): 1.0 | LR: 0.000864
Ep 0850 | Loss: 1.0687 | Steps: 1.83 | Usage(PPX): 1.0 | LR: 0.000845
Ep 0900 | Loss: 1.1498 | Steps: 1.86 | Usage(PPX): 1.0 | LR: 0.000825
Ep 0950 | Loss: 1.2948 | Steps: 1.89 | Usage(PPX): 1.0 | LR: 0.000804
Ep 1000 | Loss: 0.9906 | Steps: 1.91 | Usage(PPX): 1.0 | LR: 0.000782
Ep 1050 | Loss: 0.7563 | Steps: 1.80 | Usage(PPX): 1.0 | LR: 0.000760
Ep 1100 | Loss: 0.6577 | Steps: 1.89 | Usage(PPX): 1.0 | LR: 0.000736
Ep 1150 | Loss: 1.1126 | Steps: 1.86 | Usage(PPX): 1.0 | LR: 0.000712
Ep 1200 | Loss: 0.7503 | Steps: 1.88 | Usage(PPX): 1.0 | LR: 0.000688
Ep 1250 | Loss: 0.7474 | Steps: 1.91 | Usage(PPX): 1.0 | LR: 0.000663
Ep 1300 | Loss: 0.7194 | Steps: 1.90 | Usage(PPX): 1.0 | LR: 0.000637
Ep 1350 | Loss: 1.0606 | Steps: 1.91 | Usage(PPX): 1.0 | LR: 0.000611
Ep 1400 | Loss: 0.6693 | Steps: 1.90 | Usage(PPX): 1.0 | LR: 0.000585
Ep 1450 | Loss: 0.6215 | Steps: 1.85 | Usage(PPX): 1.0 | LR: 0.000558
Ep 1500 | Loss: 0.9688 | Steps: 1.82 | Usage(PPX): 1.0 | LR: 0.000531
Ep 1550 | Loss: 0.6825 | Steps: 1.83 | Usage(PPX): 1.0 | LR: 0.000504
Ep 1600 | Loss: 1.0784 | Steps: 1.83 | Usage(PPX): 1.0 | LR: 0.000478
Ep 1650 | Loss: 0.5681 | Steps: 1.78 | Usage(PPX): 1.0 | LR: 0.000451
Ep 1700 | Loss: 1.2201 | Steps: 1.77 | Usage(PPX): 1.0 | LR: 0.000424
Early stopping due to no improvement

--- Saving Model to crsn_omni_model.pth ---
Saved.

--- Generating Diagnostics & Images ---
Saved 1_semantic_topology.png
Running Inference Scan...
Generated: True. 
That which is above world is above is above withich is above world is be ce is ento earlike move world if it be withpe is be ce.
Thich is above is here.
Its force be.
Thich is above is above con


--- Extracting Explicit Logic Rules ---

FROM   | TO     | COUNT  | AVG STEPS 
---------------------------------------------
S_60   -> S_60   | 151    | 1.77
S_6    -> S_60   | 20     | 1.85
S_60   -> S_6    | 17     | 1.59
S_6    -> S_6    | 12     | 1.92
S_59   -> S_6    | 10     | 1.40
S_6    -> S_59   | 7      | 2.00
S_46   -> S_6    | 7      | 1.71
S_6    -> S_25   | 6      | 1.83
S_25   -> S_46   | 5      | 1.80
S_25   -> S_25   | 4      | 2.00
S_6    -> S_46   | 4      | 1.75
S_25   -> S_60   | 4      | 2.25
S_46   -> S_59   | 3      | 1.33
S_59   -> S_59   | 3      | 1.67
S_25   -> S_6    | 3      | 1.67
S_60   -> S_25   | 3      | 1.67
S_59   -> S_25   | 2      | 1.50
S_46   -> S_25   | 2      | 2.00
S_46   -> S_46   | 2      | 2.00
S_60   -> S_46   | 2      | 2.00

---  Dream Mode (Pure Symbolic Walk) ---
Dream Output: T


---  Anomaly Detection Test ---
Input: 'True without falsehood certain and most banana'
Saved 5_anomaly_detection.png

<img width="1000" height="400" alt="5_anomaly_detection" src="https://github.com/user-attachments/assets/2733cc44-53a0-47ad-b2ce-a75bf7165b09" />
<img width="800" height="800" alt="4_phase_plot" src="https://github.com/user-attachments/assets/478aa023-03b7-4f79-a0c6-0cb611067ac6" />
<img width="1200" height="400" alt="3_act_profile" src="https://github.com/user-attachments/assets/94183a5f-5892-43b1-a107-da163adf1d68" />
<img width="1200" height="400" alt="2_stack_mri" src="https://github.com/user-attachments/assets/613a639f-0f7a-4be7-a623-b72655d84a38" />
<img width="2100" height="2100" alt="1_semantic_topology" src="https://github.com/user-attachments/assets/21d5281d-618a-4ea9-82c2-679adf370b62" />

