***IT NEEDS FINETUNING TO BE PERFECT***

**SACRSN BEFORE FINE TUNING**

To scale it up it will need Holographic Associative Memory

SACRSN-SAVE-ROUND1.py

1000 EPISODES

Ep 0000 | Loss: 4.2455 | Avg Thinking Steps: 7.81
Ep 0010 | Loss: 3.3380 | Avg Thinking Steps: 5.39
Ep 0020 | Loss: 3.2582 | Avg Thinking Steps: 5.72
Ep 0030 | Loss: 3.2140 | Avg Thinking Steps: 5.06
Ep 0040 | Loss: 3.1292 | Avg Thinking Steps: 4.51
Ep 0050 | Loss: 3.0924 | Avg Thinking Steps: 4.27
Ep 0060 | Loss: 3.1051 | Avg Thinking Steps: 4.26
Ep 0070 | Loss: 3.1919 | Avg Thinking Steps: 4.03
Ep 0080 | Loss: 3.0788 | Avg Thinking Steps: 3.76
Ep 0090 | Loss: 3.1452 | Avg Thinking Steps: 4.59
Ep 0100 | Loss: 3.2858 | Avg Thinking Steps: 3.98
Ep 0110 | Loss: 3.3216 | Avg Thinking Steps: 4.20
Ep 0120 | Loss: 3.3262 | Avg Thinking Steps: 4.46
Ep 0130 | Loss: 3.2812 | Avg Thinking Steps: 4.44
Ep 0140 | Loss: 3.2720 | Avg Thinking Steps: 4.70
Ep 0150 | Loss: 3.2830 | Avg Thinking Steps: 4.32
Ep 0160 | Loss: 3.2146 | Avg Thinking Steps: 4.08
Ep 0170 | Loss: 3.1012 | Avg Thinking Steps: 3.61
Ep 0180 | Loss: 3.2110 | Avg Thinking Steps: 3.46
Ep 0190 | Loss: 3.3301 | Avg Thinking Steps: 3.69
Ep 0200 | Loss: 3.2724 | Avg Thinking Steps: 4.32
Ep 0210 | Loss: 3.3291 | Avg Thinking Steps: 3.93
Ep 0220 | Loss: 3.2821 | Avg Thinking Steps: 4.12
Ep 0230 | Loss: 3.2794 | Avg Thinking Steps: 4.18
Ep 0240 | Loss: 3.3225 | Avg Thinking Steps: 3.64
Ep 0250 | Loss: 3.3674 | Avg Thinking Steps: 3.64
Ep 0260 | Loss: 3.4597 | Avg Thinking Steps: 4.00
Ep 0270 | Loss: 3.1846 | Avg Thinking Steps: 3.95
Ep 0280 | Loss: 3.0486 | Avg Thinking Steps: 3.99
Ep 0290 | Loss: 3.4233 | Avg Thinking Steps: 4.30
Ep 0300 | Loss: 3.2570 | Avg Thinking Steps: 5.45
Ep 0310 | Loss: 3.1047 | Avg Thinking Steps: 5.79
Ep 0320 | Loss: 3.1103 | Avg Thinking Steps: 5.56
Ep 0330 | Loss: 3.0202 | Avg Thinking Steps: 4.80
Ep 0340 | Loss: 3.0734 | Avg Thinking Steps: 4.51
Ep 0350 | Loss: 3.0103 | Avg Thinking Steps: 4.96
Ep 0360 | Loss: 3.0937 | Avg Thinking Steps: 5.35
Ep 0370 | Loss: 2.9435 | Avg Thinking Steps: 5.42
Ep 0380 | Loss: 2.8566 | Avg Thinking Steps: 5.12
Ep 0390 | Loss: 2.8813 | Avg Thinking Steps: 4.59
Ep 0400 | Loss: 2.8291 | Avg Thinking Steps: 4.57
Ep 0410 | Loss: 2.2584 | Avg Thinking Steps: 4.10
Ep 0420 | Loss: 2.0204 | Avg Thinking Steps: 3.03
Ep 0430 | Loss: 1.6866 | Avg Thinking Steps: 2.76
Ep 0440 | Loss: 1.4404 | Avg Thinking Steps: 2.49
Ep 0450 | Loss: 1.4485 | Avg Thinking Steps: 2.35
Ep 0460 | Loss: 1.1686 | Avg Thinking Steps: 2.17
Ep 0470 | Loss: 1.1113 | Avg Thinking Steps: 2.20
Ep 0480 | Loss: 1.0931 | Avg Thinking Steps: 1.99
Ep 0490 | Loss: 0.8789 | Avg Thinking Steps: 1.84
Ep 0500 | Loss: 0.8568 | Avg Thinking Steps: 1.81
Ep 0510 | Loss: 0.8673 | Avg Thinking Steps: 1.74
Ep 0520 | Loss: 0.6889 | Avg Thinking Steps: 1.66
Ep 0530 | Loss: 0.8927 | Avg Thinking Steps: 1.75
Ep 0540 | Loss: 0.8648 | Avg Thinking Steps: 1.68
Ep 0550 | Loss: 0.5942 | Avg Thinking Steps: 1.62
Ep 0560 | Loss: 0.6803 | Avg Thinking Steps: 1.61
Ep 0570 | Loss: 0.6008 | Avg Thinking Steps: 1.61
Ep 0580 | Loss: 0.6761 | Avg Thinking Steps: 1.55
Ep 0590 | Loss: 0.5408 | Avg Thinking Steps: 1.50
Ep 0600 | Loss: 0.5790 | Avg Thinking Steps: 1.56
Ep 0610 | Loss: 0.6358 | Avg Thinking Steps: 1.54
Ep 0620 | Loss: 0.5107 | Avg Thinking Steps: 1.47
Ep 0630 | Loss: 0.4852 | Avg Thinking Steps: 1.47
Ep 0640 | Loss: 0.5705 | Avg Thinking Steps: 1.47
Ep 0650 | Loss: 0.5733 | Avg Thinking Steps: 1.46
Ep 0660 | Loss: 0.4587 | Avg Thinking Steps: 1.43
Ep 0670 | Loss: 0.4779 | Avg Thinking Steps: 1.40
Ep 0680 | Loss: 0.3337 | Avg Thinking Steps: 1.40
Ep 0690 | Loss: 0.5114 | Avg Thinking Steps: 1.43
Ep 0700 | Loss: 0.3907 | Avg Thinking Steps: 1.47
Ep 0710 | Loss: 0.4800 | Avg Thinking Steps: 1.47
Ep 0720 | Loss: 0.4716 | Avg Thinking Steps: 1.45
Ep 0730 | Loss: 0.3548 | Avg Thinking Steps: 1.45
Ep 0740 | Loss: 0.3599 | Avg Thinking Steps: 1.43
Ep 0750 | Loss: 0.3600 | Avg Thinking Steps: 1.45
Ep 0760 | Loss: 0.3338 | Avg Thinking Steps: 1.45
Ep 0770 | Loss: 0.4213 | Avg Thinking Steps: 1.50
Ep 0780 | Loss: 0.4751 | Avg Thinking Steps: 1.46
Ep 0790 | Loss: 0.3957 | Avg Thinking Steps: 1.43
Ep 0800 | Loss: 0.3751 | Avg Thinking Steps: 1.42
Ep 0810 | Loss: 0.2841 | Avg Thinking Steps: 1.40
Ep 0820 | Loss: 0.4040 | Avg Thinking Steps: 1.39
Ep 0830 | Loss: 0.3935 | Avg Thinking Steps: 1.39
Ep 0840 | Loss: 0.2595 | Avg Thinking Steps: 1.40
Ep 0850 | Loss: 0.2308 | Avg Thinking Steps: 1.40
Ep 0860 | Loss: 0.2622 | Avg Thinking Steps: 1.43
Ep 0870 | Loss: 0.3580 | Avg Thinking Steps: 1.42
Ep 0880 | Loss: 0.2711 | Avg Thinking Steps: 1.34
Ep 0890 | Loss: 0.3259 | Avg Thinking Steps: 1.35
Ep 0900 | Loss: 0.2719 | Avg Thinking Steps: 1.37
Ep 0910 | Loss: 0.2672 | Avg Thinking Steps: 1.38
Ep 0920 | Loss: 0.2466 | Avg Thinking Steps: 1.36
Ep 0930 | Loss: 0.2648 | Avg Thinking Steps: 1.32
Ep 0940 | Loss: 0.3150 | Avg Thinking Steps: 1.38
Ep 0950 | Loss: 0.2705 | Avg Thinking Steps: 1.38
Ep 0960 | Loss: 0.3303 | Avg Thinking Steps: 1.39
Ep 0970 | Loss: 0.2342 | Avg Thinking Steps: 1.37
Ep 0980 | Loss: 0.2579 | Avg Thinking Steps: 1.37
Ep 0990 | Loss: 0.2242 | Avg Thinking Steps: 1.36

--- Saving Model to crsn_complex_model.pth ---
Saved.
Model saved locally to /home/dan/Desktop/ProjectX/crsn_complex_model.pth

--- Visualizing Learned Symbolic Graph ---
Graph sparse.

Generated: That which is ention into earth.i
That which is below is below is above.
Its force ole  that which is

**FORCED GRAPH CREATION**
<img width="690" height="803" alt="Figure_1" src="https://github.com/user-attachments/assets/71cfa4ae-5127-4be7-95cc-8c22cea8dcf2" />

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

**SACRSN AFTER FINE TUNING**

***SACRSN-FINETUNE-ROUND2.py***

+500 MORE

Ep 1000 | Loss: 0.1332 | Avg Steps: 1.35
Ep 1010 | Loss: 0.0985 | Avg Steps: 1.36
Ep 1020 | Loss: 0.1015 | Avg Steps: 1.37
Ep 1030 | Loss: 0.0981 | Avg Steps: 1.38
Ep 1040 | Loss: 0.1061 | Avg Steps: 1.37
Ep 1050 | Loss: 0.1415 | Avg Steps: 1.38
Ep 1060 | Loss: 0.1592 | Avg Steps: 1.39
Ep 1070 | Loss: 0.0972 | Avg Steps: 1.38
Ep 1080 | Loss: 0.1892 | Avg Steps: 1.41
Ep 1090 | Loss: 0.1110 | Avg Steps: 1.40
Ep 1100 | Loss: 0.1032 | Avg Steps: 1.40
Ep 1110 | Loss: 0.1272 | Avg Steps: 1.39
Ep 1120 | Loss: 0.1054 | Avg Steps: 1.39
Ep 1130 | Loss: 0.1136 | Avg Steps: 1.40
Ep 1140 | Loss: 0.1050 | Avg Steps: 1.41
Ep 1150 | Loss: 0.1046 | Avg Steps: 1.42
Ep 1160 | Loss: 0.0966 | Avg Steps: 1.42
Ep 1170 | Loss: 0.0953 | Avg Steps: 1.45
Ep 1180 | Loss: 0.0930 | Avg Steps: 1.44
Ep 1190 | Loss: 0.1037 | Avg Steps: 1.46
Ep 1200 | Loss: 0.1776 | Avg Steps: 1.46
Ep 1210 | Loss: 0.1115 | Avg Steps: 1.45
Ep 1220 | Loss: 0.1016 | Avg Steps: 1.42
Ep 1230 | Loss: 0.0970 | Avg Steps: 1.42
Ep 1240 | Loss: 0.1022 | Avg Steps: 1.42
Ep 1250 | Loss: 0.0958 | Avg Steps: 1.45
Ep 1260 | Loss: 0.1004 | Avg Steps: 1.46
Ep 1270 | Loss: 0.0967 | Avg Steps: 1.43
Ep 1280 | Loss: 0.0892 | Avg Steps: 1.43
Ep 1290 | Loss: 0.0893 | Avg Steps: 1.44
Ep 1300 | Loss: 0.0888 | Avg Steps: 1.44
Ep 1310 | Loss: 0.0983 | Avg Steps: 1.43
Ep 1320 | Loss: 0.0933 | Avg Steps: 1.46
Ep 1330 | Loss: 0.0944 | Avg Steps: 1.46
Ep 1340 | Loss: 0.1108 | Avg Steps: 1.45
Ep 1350 | Loss: 0.1015 | Avg Steps: 1.45
Ep 1360 | Loss: 0.0984 | Avg Steps: 1.43
Ep 1370 | Loss: 0.0991 | Avg Steps: 1.44
Ep 1380 | Loss: 0.0976 | Avg Steps: 1.44
Ep 1390 | Loss: 0.0957 | Avg Steps: 1.42
Ep 1400 | Loss: 0.1047 | Avg Steps: 1.41
Ep 1410 | Loss: 0.1068 | Avg Steps: 1.42
Ep 1420 | Loss: 0.1094 | Avg Steps: 1.42
Ep 1430 | Loss: 0.0972 | Avg Steps: 1.43
Ep 1440 | Loss: 0.1069 | Avg Steps: 1.43
Ep 1450 | Loss: 0.0970 | Avg Steps: 1.42
Ep 1460 | Loss: 0.0973 | Avg Steps: 1.43
Ep 1470 | Loss: 0.1081 | Avg Steps: 1.41
Ep 1480 | Loss: 0.1329 | Avg Steps: 1.40
Ep 1490 | Loss: 0.0829 | Avg Steps: 1.39

--- Saving Model to crsn_complex_model.pth ---
Saved.

--- Visualizing Learned Symbolic Graph ---

Generated: That which is below is like to that which is above.
The  of it be world is like to that which is above.
The  t all pentir of all perthout fals is like

<img width="690" height="803" alt="Figure_2" src="https://github.com/user-attachments/assets/fe8382da-21c7-4d22-aa31-122b5e9f319b" />

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

WITH CONFIG 

"embedding_dim": 64

"n_symbols": 64

"act_threshold": 0.9999

Ep 0000 | Loss: 4.1463 | Avg Thinking Steps: 7.72
Ep 0010 | Loss: 3.2999 | Avg Thinking Steps: 5.89
Ep 0020 | Loss: 3.1952 | Avg Thinking Steps: 5.16
Ep 0030 | Loss: 3.2096 | Avg Thinking Steps: 4.80
Ep 0040 | Loss: 3.2536 | Avg Thinking Steps: 4.43
Ep 0050 | Loss: 3.1265 | Avg Thinking Steps: 4.14
Ep 0060 | Loss: 3.1977 | Avg Thinking Steps: 3.91
Ep 0070 | Loss: 3.2302 | Avg Thinking Steps: 3.97
Ep 0080 | Loss: 3.2078 | Avg Thinking Steps: 3.84
Ep 0090 | Loss: 3.3837 | Avg Thinking Steps: 3.49
Ep 0100 | Loss: 3.2556 | Avg Thinking Steps: 3.57
Ep 0110 | Loss: 3.3830 | Avg Thinking Steps: 3.89
Ep 0120 | Loss: 3.2321 | Avg Thinking Steps: 3.82
Ep 0130 | Loss: 3.2922 | Avg Thinking Steps: 3.89
Ep 0140 | Loss: 3.2203 | Avg Thinking Steps: 3.99
Ep 0150 | Loss: 3.3150 | Avg Thinking Steps: 3.85
Ep 0160 | Loss: 3.1785 | Avg Thinking Steps: 4.04
Ep 0170 | Loss: 3.2579 | Avg Thinking Steps: 4.10
Ep 0180 | Loss: 3.3079 | Avg Thinking Steps: 4.26
Ep 0190 | Loss: 3.2042 | Avg Thinking Steps: 4.18
Ep 0200 | Loss: 3.2819 | Avg Thinking Steps: 4.37
Ep 0210 | Loss: 3.2289 | Avg Thinking Steps: 3.69
Ep 0220 | Loss: 3.3474 | Avg Thinking Steps: 3.52
Ep 0230 | Loss: 3.3092 | Avg Thinking Steps: 3.53
Ep 0240 | Loss: 3.4275 | Avg Thinking Steps: 3.50
Ep 0250 | Loss: 3.3770 | Avg Thinking Steps: 3.49
Ep 0260 | Loss: 3.3584 | Avg Thinking Steps: 3.64
Ep 0270 | Loss: 3.3166 | Avg Thinking Steps: 3.74
Ep 0280 | Loss: 3.2917 | Avg Thinking Steps: 3.89
Ep 0290 | Loss: 3.4637 | Avg Thinking Steps: 4.12
Ep 0300 | Loss: 3.2821 | Avg Thinking Steps: 4.37
Ep 0310 | Loss: 3.2676 | Avg Thinking Steps: 4.49
Ep 0320 | Loss: 3.4065 | Avg Thinking Steps: 4.44
Ep 0330 | Loss: 3.1763 | Avg Thinking Steps: 4.88
Ep 0340 | Loss: 3.2152 | Avg Thinking Steps: 5.16
Ep 0350 | Loss: 3.2274 | Avg Thinking Steps: 4.61
Ep 0360 | Loss: 3.0223 | Avg Thinking Steps: 4.28
Ep 0370 | Loss: 3.0863 | Avg Thinking Steps: 4.53
Ep 0380 | Loss: 3.0290 | Avg Thinking Steps: 4.57
Ep 0390 | Loss: 2.8823 | Avg Thinking Steps: 4.04
Ep 0400 | Loss: 2.8484 | Avg Thinking Steps: 3.58
Ep 0410 | Loss: 2.8141 | Avg Thinking Steps: 3.37
Ep 0420 | Loss: 2.5425 | Avg Thinking Steps: 3.78
Ep 0430 | Loss: 2.4119 | Avg Thinking Steps: 3.46
Ep 0440 | Loss: 2.2194 | Avg Thinking Steps: 3.13
Ep 0450 | Loss: 2.1420 | Avg Thinking Steps: 2.99
Ep 0460 | Loss: 1.8607 | Avg Thinking Steps: 2.61
Ep 0470 | Loss: 1.7362 | Avg Thinking Steps: 2.40
Ep 0480 | Loss: 1.5393 | Avg Thinking Steps: 2.09
Ep 0490 | Loss: 1.5452 | Avg Thinking Steps: 1.99
Ep 0500 | Loss: 1.4882 | Avg Thinking Steps: 1.80
Ep 0510 | Loss: 1.3191 | Avg Thinking Steps: 1.87
Ep 0520 | Loss: 1.3726 | Avg Thinking Steps: 1.86
Ep 0530 | Loss: 1.2178 | Avg Thinking Steps: 1.80
Ep 0540 | Loss: 1.2705 | Avg Thinking Steps: 1.79
Ep 0550 | Loss: 1.1117 | Avg Thinking Steps: 1.69
Ep 0560 | Loss: 1.1896 | Avg Thinking Steps: 1.68
Ep 0570 | Loss: 1.0855 | Avg Thinking Steps: 1.66
Ep 0580 | Loss: 0.9837 | Avg Thinking Steps: 1.57
Ep 0590 | Loss: 0.9894 | Avg Thinking Steps: 1.61
Ep 0600 | Loss: 0.9142 | Avg Thinking Steps: 1.49
Ep 0610 | Loss: 0.8895 | Avg Thinking Steps: 1.51
Ep 0620 | Loss: 0.7123 | Avg Thinking Steps: 1.53
Ep 0630 | Loss: 0.7905 | Avg Thinking Steps: 1.50
Ep 0640 | Loss: 0.6758 | Avg Thinking Steps: 1.47
Ep 0650 | Loss: 0.8428 | Avg Thinking Steps: 1.47
Ep 0660 | Loss: 0.8095 | Avg Thinking Steps: 1.44
Ep 0670 | Loss: 0.8879 | Avg Thinking Steps: 1.39
Ep 0680 | Loss: 0.5772 | Avg Thinking Steps: 1.36
Ep 0690 | Loss: 0.6151 | Avg Thinking Steps: 1.38
Ep 0700 | Loss: 0.5386 | Avg Thinking Steps: 1.36
Ep 0710 | Loss: 0.7221 | Avg Thinking Steps: 1.40
Ep 0720 | Loss: 0.6177 | Avg Thinking Steps: 1.37
Ep 0730 | Loss: 0.3857 | Avg Thinking Steps: 1.34
Ep 0740 | Loss: 0.3522 | Avg Thinking Steps: 1.35
Ep 0750 | Loss: 0.6779 | Avg Thinking Steps: 1.34
Ep 0760 | Loss: 0.5240 | Avg Thinking Steps: 1.36
Ep 0770 | Loss: 0.4370 | Avg Thinking Steps: 1.33
Ep 0780 | Loss: 0.4510 | Avg Thinking Steps: 1.31
Ep 0790 | Loss: 0.4578 | Avg Thinking Steps: 1.29
Ep 0800 | Loss: 0.5209 | Avg Thinking Steps: 1.25
Ep 0810 | Loss: 0.4249 | Avg Thinking Steps: 1.30
Ep 0820 | Loss: 0.5609 | Avg Thinking Steps: 1.34
Ep 0830 | Loss: 0.5570 | Avg Thinking Steps: 1.27
Ep 0840 | Loss: 0.6495 | Avg Thinking Steps: 1.27
Ep 0850 | Loss: 0.4904 | Avg Thinking Steps: 1.24
Ep 0860 | Loss: 0.3993 | Avg Thinking Steps: 1.28
Ep 0870 | Loss: 0.4296 | Avg Thinking Steps: 1.24
Ep 0880 | Loss: 0.3398 | Avg Thinking Steps: 1.19
Ep 0890 | Loss: 0.3763 | Avg Thinking Steps: 1.17
Ep 0900 | Loss: 0.3979 | Avg Thinking Steps: 1.19
Ep 0910 | Loss: 0.3172 | Avg Thinking Steps: 1.14
Ep 0920 | Loss: 0.3603 | Avg Thinking Steps: 1.19
Ep 0930 | Loss: 0.3342 | Avg Thinking Steps: 1.18
Ep 0940 | Loss: 0.3476 | Avg Thinking Steps: 1.15
Ep 0950 | Loss: 0.3108 | Avg Thinking Steps: 1.18
Ep 0960 | Loss: 0.3129 | Avg Thinking Steps: 1.16
Ep 0970 | Loss: 0.2867 | Avg Thinking Steps: 1.17
Ep 0980 | Loss: 0.2550 | Avg Thinking Steps: 1.18
Ep 0990 | Loss: 0.2854 | Avg Thinking Steps: 1.16
Ep 1000 | Loss: 0.2378 | Avg Thinking Steps: 1.15
Ep 1010 | Loss: 0.4887 | Avg Thinking Steps: 1.15
Ep 1020 | Loss: 0.3846 | Avg Thinking Steps: 1.16
Ep 1030 | Loss: 0.2437 | Avg Thinking Steps: 1.15
Ep 1040 | Loss: 0.4449 | Avg Thinking Steps: 1.15
Ep 1050 | Loss: 0.2638 | Avg Thinking Steps: 1.16
Ep 1060 | Loss: 0.2433 | Avg Thinking Steps: 1.18
Ep 1070 | Loss: 0.2647 | Avg Thinking Steps: 1.19
Ep 1080 | Loss: 0.3438 | Avg Thinking Steps: 1.16
Ep 1090 | Loss: 0.2386 | Avg Thinking Steps: 1.17
Ep 1100 | Loss: 0.2360 | Avg Thinking Steps: 1.14
Ep 1110 | Loss: 0.3319 | Avg Thinking Steps: 1.15
Ep 1120 | Loss: 0.3374 | Avg Thinking Steps: 1.18
Ep 1130 | Loss: 0.2241 | Avg Thinking Steps: 1.12
Ep 1140 | Loss: 0.2130 | Avg Thinking Steps: 1.13
Ep 1150 | Loss: 0.2242 | Avg Thinking Steps: 1.15
Ep 1160 | Loss: 0.3069 | Avg Thinking Steps: 1.14
Ep 1170 | Loss: 0.2664 | Avg Thinking Steps: 1.12
Ep 1180 | Loss: 0.2081 | Avg Thinking Steps: 1.16
Ep 1190 | Loss: 0.2116 | Avg Thinking Steps: 1.16
Ep 1200 | Loss: 0.1743 | Avg Thinking Steps: 1.15
Ep 1210 | Loss: 0.3965 | Avg Thinking Steps: 1.18
Ep 1220 | Loss: 0.2694 | Avg Thinking Steps: 1.18
Ep 1230 | Loss: 0.2684 | Avg Thinking Steps: 1.17
Ep 1240 | Loss: 0.2106 | Avg Thinking Steps: 1.13
Ep 1250 | Loss: 0.2049 | Avg Thinking Steps: 1.14
Ep 1260 | Loss: 0.1845 | Avg Thinking Steps: 1.12
Ep 1270 | Loss: 0.1907 | Avg Thinking Steps: 1.12
Ep 1280 | Loss: 0.1977 | Avg Thinking Steps: 1.12
Ep 1290 | Loss: 0.2752 | Avg Thinking Steps: 1.12
Ep 1300 | Loss: 0.2416 | Avg Thinking Steps: 1.13
Ep 1310 | Loss: 0.1528 | Avg Thinking Steps: 1.12
Ep 1320 | Loss: 0.1764 | Avg Thinking Steps: 1.13
Ep 1330 | Loss: 0.2057 | Avg Thinking Steps: 1.16
Ep 1340 | Loss: 0.1932 | Avg Thinking Steps: 1.13
Ep 1350 | Loss: 0.2026 | Avg Thinking Steps: 1.12
Ep 1360 | Loss: 0.2182 | Avg Thinking Steps: 1.13
Ep 1370 | Loss: 0.1782 | Avg Thinking Steps: 1.13
Ep 1380 | Loss: 0.1478 | Avg Thinking Steps: 1.13
Ep 1390 | Loss: 0.1615 | Avg Thinking Steps: 1.16
Ep 1400 | Loss: 0.1776 | Avg Thinking Steps: 1.16
Ep 1410 | Loss: 0.2250 | Avg Thinking Steps: 1.15
Ep 1420 | Loss: 0.2510 | Avg Thinking Steps: 1.16
Ep 1430 | Loss: 0.1701 | Avg Thinking Steps: 1.15
Ep 1440 | Loss: 0.1741 | Avg Thinking Steps: 1.14
Ep 1450 | Loss: 0.1324 | Avg Thinking Steps: 1.16
Ep 1460 | Loss: 0.2329 | Avg Thinking Steps: 1.19
Ep 1470 | Loss: 0.1498 | Avg Thinking Steps: 1.19
Ep 1480 | Loss: 0.2097 | Avg Thinking Steps: 1.13
Ep 1490 | Loss: 0.1910 | Avg Thinking Steps: 1.14
Ep 1500 | Loss: 0.2217 | Avg Thinking Steps: 1.17
Ep 1510 | Loss: 0.1543 | Avg Thinking Steps: 1.18
Ep 1520 | Loss: 0.2496 | Avg Thinking Steps: 1.19
Ep 1530 | Loss: 0.1385 | Avg Thinking Steps: 1.16
Ep 1540 | Loss: 0.3616 | Avg Thinking Steps: 1.16
Ep 1550 | Loss: 0.2149 | Avg Thinking Steps: 1.19
Ep 1560 | Loss: 0.1697 | Avg Thinking Steps: 1.18
Ep 1570 | Loss: 0.1857 | Avg Thinking Steps: 1.17
Ep 1580 | Loss: 0.2507 | Avg Thinking Steps: 1.14
Ep 1590 | Loss: 0.1713 | Avg Thinking Steps: 1.16
Ep 1600 | Loss: 0.1334 | Avg Thinking Steps: 1.16
Ep 1610 | Loss: 0.2253 | Avg Thinking Steps: 1.14
Ep 1620 | Loss: 0.2330 | Avg Thinking Steps: 1.13
Ep 1630 | Loss: 0.2012 | Avg Thinking Steps: 1.16
Ep 1640 | Loss: 0.2925 | Avg Thinking Steps: 1.17
Ep 1650 | Loss: 0.2042 | Avg Thinking Steps: 1.13
Ep 1660 | Loss: 0.1959 | Avg Thinking Steps: 1.14
Ep 1670 | Loss: 0.1577 | Avg Thinking Steps: 1.14
Ep 1680 | Loss: 0.1777 | Avg Thinking Steps: 1.15
Ep 1690 | Loss: 0.2823 | Avg Thinking Steps: 1.15
Ep 1700 | Loss: 0.1729 | Avg Thinking Steps: 1.14
Ep 1710 | Loss: 0.2550 | Avg Thinking Steps: 1.13
Ep 1720 | Loss: 0.1162 | Avg Thinking Steps: 1.16
Ep 1730 | Loss: 0.1342 | Avg Thinking Steps: 1.18
Ep 1740 | Loss: 0.1492 | Avg Thinking Steps: 1.17
Ep 1750 | Loss: 0.2259 | Avg Thinking Steps: 1.15
Ep 1760 | Loss: 0.2343 | Avg Thinking Steps: 1.18
Ep 1770 | Loss: 0.1889 | Avg Thinking Steps: 1.16
Ep 1780 | Loss: 0.1986 | Avg Thinking Steps: 1.14
Ep 1790 | Loss: 0.1844 | Avg Thinking Steps: 1.12
Ep 1800 | Loss: 0.3049 | Avg Thinking Steps: 1.17
Ep 1810 | Loss: 0.1718 | Avg Thinking Steps: 1.16
Ep 1820 | Loss: 0.1268 | Avg Thinking Steps: 1.16
Ep 1830 | Loss: 0.2173 | Avg Thinking Steps: 1.17
Ep 1840 | Loss: 0.1749 | Avg Thinking Steps: 1.16
Ep 1850 | Loss: 0.2013 | Avg Thinking Steps: 1.16
Ep 1860 | Loss: 0.2058 | Avg Thinking Steps: 1.20
Ep 1870 | Loss: 0.2986 | Avg Thinking Steps: 1.18
Ep 1880 | Loss: 0.1773 | Avg Thinking Steps: 1.15
Ep 1890 | Loss: 0.1582 | Avg Thinking Steps: 1.18
Ep 1900 | Loss: 0.1876 | Avg Thinking Steps: 1.18
Ep 1910 | Loss: 0.2478 | Avg Thinking Steps: 1.18
Ep 1920 | Loss: 0.1592 | Avg Thinking Steps: 1.15
Ep 1930 | Loss: 0.3353 | Avg Thinking Steps: 1.16
Ep 1940 | Loss: 0.1335 | Avg Thinking Steps: 1.16
Ep 1950 | Loss: 0.1265 | Avg Thinking Steps: 1.15
Ep 1960 | Loss: 0.1640 | Avg Thinking Steps: 1.16
Ep 1970 | Loss: 0.1897 | Avg Thinking Steps: 1.15
Ep 1980 | Loss: 0.2445 | Avg Thinking Steps: 1.14
Ep 1990 | Loss: 0.1571 | Avg Thinking Steps: 1.18
Ep 2000 | Loss: 0.1958 | Avg Thinking Steps: 1.17
Ep 2010 | Loss: 0.1716 | Avg Thinking Steps: 1.15
Ep 2020 | Loss: 0.2643 | Avg Thinking Steps: 1.16
Ep 2030 | Loss: 0.2856 | Avg Thinking Steps: 1.16
Ep 2040 | Loss: 0.2441 | Avg Thinking Steps: 1.16
Ep 2050 | Loss: 0.1810 | Avg Thinking Steps: 1.16
Ep 2060 | Loss: 0.2133 | Avg Thinking Steps: 1.16
Ep 2070 | Loss: 0.2883 | Avg Thinking Steps: 1.16
Ep 2080 | Loss: 0.2229 | Avg Thinking Steps: 1.13
Ep 2090 | Loss: 0.1826 | Avg Thinking Steps: 1.14
Ep 2100 | Loss: 0.1400 | Avg Thinking Steps: 1.16
Ep 2110 | Loss: 0.1458 | Avg Thinking Steps: 1.14
Ep 2120 | Loss: 0.1946 | Avg Thinking Steps: 1.14
Ep 2130 | Loss: 0.1935 | Avg Thinking Steps: 1.13
Ep 2140 | Loss: 0.2591 | Avg Thinking Steps: 1.17
Ep 2150 | Loss: 0.1648 | Avg Thinking Steps: 1.14
Ep 2160 | Loss: 0.1599 | Avg Thinking Steps: 1.14
Ep 2170 | Loss: 0.2571 | Avg Thinking Steps: 1.14
Ep 2180 | Loss: 0.1660 | Avg Thinking Steps: 1.15
Ep 2190 | Loss: 0.2333 | Avg Thinking Steps: 1.12
Ep 2200 | Loss: 0.2301 | Avg Thinking Steps: 1.16
Ep 2210 | Loss: 0.1918 | Avg Thinking Steps: 1.13
Ep 2220 | Loss: 0.2368 | Avg Thinking Steps: 1.14
Ep 2230 | Loss: 0.3319 | Avg Thinking Steps: 1.16
Ep 2240 | Loss: 0.2509 | Avg Thinking Steps: 1.15
Ep 2250 | Loss: 0.1536 | Avg Thinking Steps: 1.13
Ep 2260 | Loss: 0.2584 | Avg Thinking Steps: 1.12
Ep 2270 | Loss: 0.2086 | Avg Thinking Steps: 1.15
Ep 2280 | Loss: 0.2886 | Avg Thinking Steps: 1.15
Ep 2290 | Loss: 0.1643 | Avg Thinking Steps: 1.14
Ep 2300 | Loss: 0.2972 | Avg Thinking Steps: 1.18
Ep 2310 | Loss: 0.2425 | Avg Thinking Steps: 1.18
Ep 2320 | Loss: 0.1373 | Avg Thinking Steps: 1.14
Ep 2330 | Loss: 0.2105 | Avg Thinking Steps: 1.12
Ep 2340 | Loss: 0.1733 | Avg Thinking Steps: 1.12
Ep 2350 | Loss: 0.2441 | Avg Thinking Steps: 1.14
Ep 2360 | Loss: 0.1575 | Avg Thinking Steps: 1.16
Ep 2370 | Loss: 0.1109 | Avg Thinking Steps: 1.12
Ep 2380 | Loss: 0.2289 | Avg Thinking Steps: 1.12
Ep 2390 | Loss: 0.1766 | Avg Thinking Steps: 1.15
Ep 2400 | Loss: 0.1207 | Avg Thinking Steps: 1.14
Ep 2410 | Loss: 0.1487 | Avg Thinking Steps: 1.13
Ep 2420 | Loss: 0.2315 | Avg Thinking Steps: 1.15
Ep 2430 | Loss: 0.1939 | Avg Thinking Steps: 1.12
Ep 2440 | Loss: 0.3793 | Avg Thinking Steps: 1.13
Ep 2450 | Loss: 0.1695 | Avg Thinking Steps: 1.14
Ep 2460 | Loss: 0.2461 | Avg Thinking Steps: 1.15
Ep 2470 | Loss: 0.1785 | Avg Thinking Steps: 1.16
Ep 2480 | Loss: 0.3012 | Avg Thinking Steps: 1.15
Ep 2490 | Loss: 0.1884 | Avg Thinking Steps: 1.16
Ep 2500 | Loss: 0.1743 | Avg Thinking Steps: 1.16
Ep 2510 | Loss: 0.1435 | Avg Thinking Steps: 1.14
Ep 2520 | Loss: 0.1730 | Avg Thinking Steps: 1.15
Ep 2530 | Loss: 0.1073 | Avg Thinking Steps: 1.11
Ep 2540 | Loss: 0.1503 | Avg Thinking Steps: 1.14
Ep 2550 | Loss: 0.2048 | Avg Thinking Steps: 1.13
Ep 2560 | Loss: 0.4428 | Avg Thinking Steps: 1.12
Ep 2570 | Loss: 0.1777 | Avg Thinking Steps: 1.12
Ep 2580 | Loss: 0.2939 | Avg Thinking Steps: 1.15
Ep 2590 | Loss: 0.2735 | Avg Thinking Steps: 1.16
Ep 2600 | Loss: 0.1891 | Avg Thinking Steps: 1.15
Ep 2610 | Loss: 0.2162 | Avg Thinking Steps: 1.11
Ep 2620 | Loss: 0.2425 | Avg Thinking Steps: 1.14
Ep 2630 | Loss: 0.2042 | Avg Thinking Steps: 1.13
Ep 2640 | Loss: 0.2316 | Avg Thinking Steps: 1.12
Ep 2650 | Loss: 0.2120 | Avg Thinking Steps: 1.13
Ep 2660 | Loss: 0.2342 | Avg Thinking Steps: 1.13
Ep 2670 | Loss: 0.1446 | Avg Thinking Steps: 1.12
Ep 2680 | Loss: 0.1432 | Avg Thinking Steps: 1.13
Ep 2690 | Loss: 0.1535 | Avg Thinking Steps: 1.12
Ep 2700 | Loss: 0.2174 | Avg Thinking Steps: 1.18
Ep 2710 | Loss: 0.2573 | Avg Thinking Steps: 1.12
Ep 2720 | Loss: 0.1409 | Avg Thinking Steps: 1.11
Ep 2730 | Loss: 0.1872 | Avg Thinking Steps: 1.14
Ep 2740 | Loss: 0.2407 | Avg Thinking Steps: 1.13
Ep 2750 | Loss: 0.2229 | Avg Thinking Steps: 1.11
Ep 2760 | Loss: 0.1515 | Avg Thinking Steps: 1.11
Ep 2770 | Loss: 0.1928 | Avg Thinking Steps: 1.09
Ep 2780 | Loss: 0.1744 | Avg Thinking Steps: 1.13
Ep 2790 | Loss: 0.2417 | Avg Thinking Steps: 1.13
Ep 2800 | Loss: 0.2871 | Avg Thinking Steps: 1.12
Ep 2810 | Loss: 0.2519 | Avg Thinking Steps: 1.16
Ep 2820 | Loss: 0.1498 | Avg Thinking Steps: 1.15
Ep 2830 | Loss: 0.2401 | Avg Thinking Steps: 1.16
Ep 2840 | Loss: 0.1819 | Avg Thinking Steps: 1.16
Ep 2850 | Loss: 0.2822 | Avg Thinking Steps: 1.14
Ep 2860 | Loss: 0.1970 | Avg Thinking Steps: 1.18
Ep 2870 | Loss: 0.2623 | Avg Thinking Steps: 1.18
Ep 2880 | Loss: 0.4636 | Avg Thinking Steps: 1.17
Ep 2890 | Loss: 0.2438 | Avg Thinking Steps: 1.14
Ep 2900 | Loss: 0.2291 | Avg Thinking Steps: 1.16
Ep 2910 | Loss: 0.1788 | Avg Thinking Steps: 1.18
Ep 2920 | Loss: 0.3098 | Avg Thinking Steps: 1.18
Ep 2930 | Loss: 0.3122 | Avg Thinking Steps: 1.19
Ep 2940 | Loss: 0.1694 | Avg Thinking Steps: 1.14
Ep 2950 | Loss: 0.1698 | Avg Thinking Steps: 1.15
Ep 2960 | Loss: 0.3085 | Avg Thinking Steps: 1.15
Ep 2970 | Loss: 0.3880 | Avg Thinking Steps: 1.13
Ep 2980 | Loss: 0.2262 | Avg Thinking Steps: 1.14
Ep 2990 | Loss: 0.1784 | Avg Thinking Steps: 1.12

--- Saving Model to crsn_complex_model.pth ---
Saved.
Model saved locally to /home/dan/Desktop/ProjectX/crsn_complex_model.pth

--- Visualizing Learned Symbolic Graph ---
Graph sparse.

Generated: That which is below is he is hich is above.
The is like to that which is above.
The in the is like to


WITH CONFIG

    "seq_len": 32,
    "embedding_dim": 64,
    "max_recursion_depth": 8,
    "act_threshold": 0.9999,
    "ponder_penalty": 0.0001,  
    
    "n_symbols": 64,
    "n_concepts": 8,
    "commitment_cost": 0.25,
    "graph_bias_scale": 0.5, 
    
    "epochs": 3000,
    "learning_rate": 0.001,   
    "grad_clip": 0.5,         
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "eps": 1e-6
