***IT NEEDS FINETUNING TO BE PERFECT***

**SACRSN BEFORE FINE TUNING**

To scale it up it will need Holographic Associative Memory

SACRSN-SAVE-ROUND1.py

1000 EPISODES

Ep 0000 | Loss: 4.2455 | Avg Thinking Steps: 7.81
Ep 0010 | Loss: 3.3380 | Avg Thinking Steps: 5.39
Ep 0020 | Loss: 3.2582 | Avg Thinking Steps: 5.72
Ep 0030 | Loss: 3.2140 | Avg Thinking Steps: 5.06
Ep 0040 | Loss: 3.1292 | Avg Thinking Steps: 4.51
Ep 0050 | Loss: 3.0924 | Avg Thinking Steps: 4.27
Ep 0060 | Loss: 3.1051 | Avg Thinking Steps: 4.26
Ep 0070 | Loss: 3.1919 | Avg Thinking Steps: 4.03
Ep 0080 | Loss: 3.0788 | Avg Thinking Steps: 3.76
Ep 0090 | Loss: 3.1452 | Avg Thinking Steps: 4.59
Ep 0100 | Loss: 3.2858 | Avg Thinking Steps: 3.98
Ep 0110 | Loss: 3.3216 | Avg Thinking Steps: 4.20
Ep 0120 | Loss: 3.3262 | Avg Thinking Steps: 4.46
Ep 0130 | Loss: 3.2812 | Avg Thinking Steps: 4.44
Ep 0140 | Loss: 3.2720 | Avg Thinking Steps: 4.70
Ep 0150 | Loss: 3.2830 | Avg Thinking Steps: 4.32
Ep 0160 | Loss: 3.2146 | Avg Thinking Steps: 4.08
Ep 0170 | Loss: 3.1012 | Avg Thinking Steps: 3.61
Ep 0180 | Loss: 3.2110 | Avg Thinking Steps: 3.46
Ep 0190 | Loss: 3.3301 | Avg Thinking Steps: 3.69
Ep 0200 | Loss: 3.2724 | Avg Thinking Steps: 4.32
Ep 0210 | Loss: 3.3291 | Avg Thinking Steps: 3.93
Ep 0220 | Loss: 3.2821 | Avg Thinking Steps: 4.12
Ep 0230 | Loss: 3.2794 | Avg Thinking Steps: 4.18
Ep 0240 | Loss: 3.3225 | Avg Thinking Steps: 3.64
Ep 0250 | Loss: 3.3674 | Avg Thinking Steps: 3.64
Ep 0260 | Loss: 3.4597 | Avg Thinking Steps: 4.00
Ep 0270 | Loss: 3.1846 | Avg Thinking Steps: 3.95
Ep 0280 | Loss: 3.0486 | Avg Thinking Steps: 3.99
Ep 0290 | Loss: 3.4233 | Avg Thinking Steps: 4.30
Ep 0300 | Loss: 3.2570 | Avg Thinking Steps: 5.45
Ep 0310 | Loss: 3.1047 | Avg Thinking Steps: 5.79
Ep 0320 | Loss: 3.1103 | Avg Thinking Steps: 5.56
Ep 0330 | Loss: 3.0202 | Avg Thinking Steps: 4.80
Ep 0340 | Loss: 3.0734 | Avg Thinking Steps: 4.51
Ep 0350 | Loss: 3.0103 | Avg Thinking Steps: 4.96
Ep 0360 | Loss: 3.0937 | Avg Thinking Steps: 5.35
Ep 0370 | Loss: 2.9435 | Avg Thinking Steps: 5.42
Ep 0380 | Loss: 2.8566 | Avg Thinking Steps: 5.12
Ep 0390 | Loss: 2.8813 | Avg Thinking Steps: 4.59
Ep 0400 | Loss: 2.8291 | Avg Thinking Steps: 4.57
Ep 0410 | Loss: 2.2584 | Avg Thinking Steps: 4.10
Ep 0420 | Loss: 2.0204 | Avg Thinking Steps: 3.03
Ep 0430 | Loss: 1.6866 | Avg Thinking Steps: 2.76
Ep 0440 | Loss: 1.4404 | Avg Thinking Steps: 2.49
Ep 0450 | Loss: 1.4485 | Avg Thinking Steps: 2.35
Ep 0460 | Loss: 1.1686 | Avg Thinking Steps: 2.17
Ep 0470 | Loss: 1.1113 | Avg Thinking Steps: 2.20
Ep 0480 | Loss: 1.0931 | Avg Thinking Steps: 1.99
Ep 0490 | Loss: 0.8789 | Avg Thinking Steps: 1.84
Ep 0500 | Loss: 0.8568 | Avg Thinking Steps: 1.81
Ep 0510 | Loss: 0.8673 | Avg Thinking Steps: 1.74
Ep 0520 | Loss: 0.6889 | Avg Thinking Steps: 1.66
Ep 0530 | Loss: 0.8927 | Avg Thinking Steps: 1.75
Ep 0540 | Loss: 0.8648 | Avg Thinking Steps: 1.68
Ep 0550 | Loss: 0.5942 | Avg Thinking Steps: 1.62
Ep 0560 | Loss: 0.6803 | Avg Thinking Steps: 1.61
Ep 0570 | Loss: 0.6008 | Avg Thinking Steps: 1.61
Ep 0580 | Loss: 0.6761 | Avg Thinking Steps: 1.55
Ep 0590 | Loss: 0.5408 | Avg Thinking Steps: 1.50
Ep 0600 | Loss: 0.5790 | Avg Thinking Steps: 1.56
Ep 0610 | Loss: 0.6358 | Avg Thinking Steps: 1.54
Ep 0620 | Loss: 0.5107 | Avg Thinking Steps: 1.47
Ep 0630 | Loss: 0.4852 | Avg Thinking Steps: 1.47
Ep 0640 | Loss: 0.5705 | Avg Thinking Steps: 1.47
Ep 0650 | Loss: 0.5733 | Avg Thinking Steps: 1.46
Ep 0660 | Loss: 0.4587 | Avg Thinking Steps: 1.43
Ep 0670 | Loss: 0.4779 | Avg Thinking Steps: 1.40
Ep 0680 | Loss: 0.3337 | Avg Thinking Steps: 1.40
Ep 0690 | Loss: 0.5114 | Avg Thinking Steps: 1.43
Ep 0700 | Loss: 0.3907 | Avg Thinking Steps: 1.47
Ep 0710 | Loss: 0.4800 | Avg Thinking Steps: 1.47
Ep 0720 | Loss: 0.4716 | Avg Thinking Steps: 1.45
Ep 0730 | Loss: 0.3548 | Avg Thinking Steps: 1.45
Ep 0740 | Loss: 0.3599 | Avg Thinking Steps: 1.43
Ep 0750 | Loss: 0.3600 | Avg Thinking Steps: 1.45
Ep 0760 | Loss: 0.3338 | Avg Thinking Steps: 1.45
Ep 0770 | Loss: 0.4213 | Avg Thinking Steps: 1.50
Ep 0780 | Loss: 0.4751 | Avg Thinking Steps: 1.46
Ep 0790 | Loss: 0.3957 | Avg Thinking Steps: 1.43
Ep 0800 | Loss: 0.3751 | Avg Thinking Steps: 1.42
Ep 0810 | Loss: 0.2841 | Avg Thinking Steps: 1.40
Ep 0820 | Loss: 0.4040 | Avg Thinking Steps: 1.39
Ep 0830 | Loss: 0.3935 | Avg Thinking Steps: 1.39
Ep 0840 | Loss: 0.2595 | Avg Thinking Steps: 1.40
Ep 0850 | Loss: 0.2308 | Avg Thinking Steps: 1.40
Ep 0860 | Loss: 0.2622 | Avg Thinking Steps: 1.43
Ep 0870 | Loss: 0.3580 | Avg Thinking Steps: 1.42
Ep 0880 | Loss: 0.2711 | Avg Thinking Steps: 1.34
Ep 0890 | Loss: 0.3259 | Avg Thinking Steps: 1.35
Ep 0900 | Loss: 0.2719 | Avg Thinking Steps: 1.37
Ep 0910 | Loss: 0.2672 | Avg Thinking Steps: 1.38
Ep 0920 | Loss: 0.2466 | Avg Thinking Steps: 1.36
Ep 0930 | Loss: 0.2648 | Avg Thinking Steps: 1.32
Ep 0940 | Loss: 0.3150 | Avg Thinking Steps: 1.38
Ep 0950 | Loss: 0.2705 | Avg Thinking Steps: 1.38
Ep 0960 | Loss: 0.3303 | Avg Thinking Steps: 1.39
Ep 0970 | Loss: 0.2342 | Avg Thinking Steps: 1.37
Ep 0980 | Loss: 0.2579 | Avg Thinking Steps: 1.37
Ep 0990 | Loss: 0.2242 | Avg Thinking Steps: 1.36

--- Saving Model to crsn_complex_model.pth ---
Saved.
Model saved locally to /home/dan/Desktop/ProjectX/crsn_complex_model.pth

--- Visualizing Learned Symbolic Graph ---
Graph sparse.

Generated: That which is ention into earth.i
That which is below is below is above.
Its force ole  that which is

**FORCED GRAPH CREATION**
<img width="690" height="803" alt="Figure_1" src="https://github.com/user-attachments/assets/71cfa4ae-5127-4be7-95cc-8c22cea8dcf2" />

**SACRSN AFTER FINE TUNING**

***SACRSN-FINETUNE-ROUND2.py***

+500 MORE

Ep 1000 | Loss: 0.1332 | Avg Steps: 1.35
Ep 1010 | Loss: 0.0985 | Avg Steps: 1.36
Ep 1020 | Loss: 0.1015 | Avg Steps: 1.37
Ep 1030 | Loss: 0.0981 | Avg Steps: 1.38
Ep 1040 | Loss: 0.1061 | Avg Steps: 1.37
Ep 1050 | Loss: 0.1415 | Avg Steps: 1.38
Ep 1060 | Loss: 0.1592 | Avg Steps: 1.39
Ep 1070 | Loss: 0.0972 | Avg Steps: 1.38
Ep 1080 | Loss: 0.1892 | Avg Steps: 1.41
Ep 1090 | Loss: 0.1110 | Avg Steps: 1.40
Ep 1100 | Loss: 0.1032 | Avg Steps: 1.40
Ep 1110 | Loss: 0.1272 | Avg Steps: 1.39
Ep 1120 | Loss: 0.1054 | Avg Steps: 1.39
Ep 1130 | Loss: 0.1136 | Avg Steps: 1.40
Ep 1140 | Loss: 0.1050 | Avg Steps: 1.41
Ep 1150 | Loss: 0.1046 | Avg Steps: 1.42
Ep 1160 | Loss: 0.0966 | Avg Steps: 1.42
Ep 1170 | Loss: 0.0953 | Avg Steps: 1.45
Ep 1180 | Loss: 0.0930 | Avg Steps: 1.44
Ep 1190 | Loss: 0.1037 | Avg Steps: 1.46
Ep 1200 | Loss: 0.1776 | Avg Steps: 1.46
Ep 1210 | Loss: 0.1115 | Avg Steps: 1.45
Ep 1220 | Loss: 0.1016 | Avg Steps: 1.42
Ep 1230 | Loss: 0.0970 | Avg Steps: 1.42
Ep 1240 | Loss: 0.1022 | Avg Steps: 1.42
Ep 1250 | Loss: 0.0958 | Avg Steps: 1.45
Ep 1260 | Loss: 0.1004 | Avg Steps: 1.46
Ep 1270 | Loss: 0.0967 | Avg Steps: 1.43
Ep 1280 | Loss: 0.0892 | Avg Steps: 1.43
Ep 1290 | Loss: 0.0893 | Avg Steps: 1.44
Ep 1300 | Loss: 0.0888 | Avg Steps: 1.44
Ep 1310 | Loss: 0.0983 | Avg Steps: 1.43
Ep 1320 | Loss: 0.0933 | Avg Steps: 1.46
Ep 1330 | Loss: 0.0944 | Avg Steps: 1.46
Ep 1340 | Loss: 0.1108 | Avg Steps: 1.45
Ep 1350 | Loss: 0.1015 | Avg Steps: 1.45
Ep 1360 | Loss: 0.0984 | Avg Steps: 1.43
Ep 1370 | Loss: 0.0991 | Avg Steps: 1.44
Ep 1380 | Loss: 0.0976 | Avg Steps: 1.44
Ep 1390 | Loss: 0.0957 | Avg Steps: 1.42
Ep 1400 | Loss: 0.1047 | Avg Steps: 1.41
Ep 1410 | Loss: 0.1068 | Avg Steps: 1.42
Ep 1420 | Loss: 0.1094 | Avg Steps: 1.42
Ep 1430 | Loss: 0.0972 | Avg Steps: 1.43
Ep 1440 | Loss: 0.1069 | Avg Steps: 1.43
Ep 1450 | Loss: 0.0970 | Avg Steps: 1.42
Ep 1460 | Loss: 0.0973 | Avg Steps: 1.43
Ep 1470 | Loss: 0.1081 | Avg Steps: 1.41
Ep 1480 | Loss: 0.1329 | Avg Steps: 1.40
Ep 1490 | Loss: 0.0829 | Avg Steps: 1.39

--- Saving Model to crsn_complex_model.pth ---
Saved.

--- Visualizing Learned Symbolic Graph ---

Generated: That which is below is like to that which is above.
The  of it be world is like to that which is above.
The  t all pentir of all perthout fals is like

<img width="690" height="803" alt="Figure_2" src="https://github.com/user-attachments/assets/fe8382da-21c7-4d22-aa31-122b5e9f319b" />


